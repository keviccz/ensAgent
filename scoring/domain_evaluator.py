import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Any
# from langchain.llms import OpenAI
# from langchain.chains import RetrievalQA
from gpt_scorer import GPTDomainScorer, RUBRIC_TEXT, GLOSSARY_DEFAULT, TASK_DESCRIPTION, DEG_GLOSSARY
from pathway_analyzer import PathwayAnalyzer
from visual_score_integrator import VisualScoreIntegrator
# ... å…¶ä»–å¿…è¦çš„å¯¼å…¥

# ä¸»ç±»ï¼šDomainEvaluator
class DomainEvaluator:
    def __init__(
        self,
        openai_api_key: str,
        output_format: str = 'json',
        api_provider: str = "",
        api_key: str = "",
        api_endpoint: str = "",
        api_model: str = "",
        api_version: str = "",
        azure_endpoint: str = None,
        azure_deployment: str = None,
        azure_api_version: str = None,
        temperature: float = 0.2,
        max_completion_tokens: int = None,
        top_p: float = 1.0,
        frequency_penalty: float = 0.0,
        presence_penalty: float = 0.0,
        top_n_deg: int = 5,
        example_db: str = None,
        use_example_db: bool = False,
        knn_k: int = 5,
        enforce_discrimination: bool = False,
    ):
        """
        åˆå§‹åŒ–DomainEvaluator
        :param openai_api_key: OpenAI APIå¯†é’¥
        :param output_format: è¾“å‡ºæ ¼å¼ï¼ˆjson, dataframe, sqlï¼‰
        :param azure_endpoint: Azure OpenAI endpoint
        :param azure_deployment: Azure OpenAI deploymentåç§°
        :param azure_api_version: Azure OpenAI APIç‰ˆæœ¬
        :param temperature: å¤§æ¨¡å‹temperatureå‚æ•°
        :param top_n_deg: æ¯ä¸ªdomainæœ€å¤šè€ƒè™‘çš„degæ•°é‡
        :param example_db: ç¤ºä¾‹æ•°æ®åº“
        :param use_example_db: æ˜¯å¦ä½¿ç”¨ç¤ºä¾‹æ•°æ®åº“
        :param knn_k: ç”¨äºæ£€ç´¢ç¤ºä¾‹çš„KNNå‚æ•°
        """
        self.openai_api_key = openai_api_key
        self.output_format = output_format
        self.gpt_scorer = GPTDomainScorer(
            openai_api_key=api_key or openai_api_key,
            api_provider=api_provider,
            api_key=api_key or openai_api_key,
            api_endpoint=api_endpoint or azure_endpoint,
            api_model=api_model or azure_deployment,
            api_version=api_version or azure_api_version,
            azure_endpoint=azure_endpoint,
            azure_deployment=azure_deployment,
            azure_api_version=azure_api_version,
            temperature=temperature,
            max_completion_tokens=max_completion_tokens,
            top_p=top_p,
            frequency_penalty=frequency_penalty,
            presence_penalty=presence_penalty,
            enforce_discrimination=enforce_discrimination
        )
        # åˆå§‹åŒ–Langchainæ£€ç´¢é“¾ï¼ˆå¯æ ¹æ®åç»­éœ€è¦æ‰©å±•ï¼‰
        self.retriever = None  # è¿™é‡Œåç»­å¯æ¥å…¥è‡ªå®šä¹‰çŸ¥è¯†åº“
        self.top_n_deg = top_n_deg
        self.example_db = example_db
        self.use_example_db = use_example_db
        self.knn_k = knn_k
        
        # ç¤ºä¾‹åº“åŠŸèƒ½å·²ç§»é™¤ï¼Œé¿å…ground truthæˆ–ç¤ºä¾‹æ³„æ¼
        self.example_store = None
        
        # åˆå§‹åŒ–pathwayåˆ†æå™¨
        self.pathway_analyzer = PathwayAnalyzer()
        self.pathway_data = None
        self.pathway_scores = None
        
        # åˆå§‹åŒ–è§†è§‰è¯„åˆ†é›†æˆå™¨
        self.visual_integrator = VisualScoreIntegrator()
        self.visual_scores = None
        self.use_visual_integration = False

    def load_pathway_data(self, input_dir: str, method_name: str):
        """
        åŠ è½½ç‰¹å®šæ–¹æ³•çš„pathwayæ•°æ®
        :param input_dir: è¾“å…¥ç›®å½•
        :param method_name: æ–¹æ³•åç§°
        """
        try:
            # åŠ è½½æ‰€æœ‰pathwayæ•°æ®
            all_pathway_data = self.pathway_analyzer.load_pathway_data(input_dir)
            
            if method_name in all_pathway_data:
                self.pathway_data = {method_name: all_pathway_data[method_name]}
                # åˆ†æpathwayå¯Œé›†è´¨é‡
                self.pathway_scores = self.pathway_analyzer.analyze_pathway_enrichment(self.pathway_data)
                print(f"[Info] æˆåŠŸåŠ è½½ {method_name} çš„pathwayæ•°æ®")
            else:
                print(f"[Warning] æœªæ‰¾åˆ° {method_name} çš„pathwayæ•°æ®")
                self.pathway_data = None
                self.pathway_scores = None
        except Exception as e:
            print(f"[Warning] åŠ è½½pathwayæ•°æ®å¤±è´¥: {e}")
            self.pathway_data = None
            self.pathway_scores = None

    def load_visual_scores(self, pic_analyze_dir: str = "pic_analyze") -> bool:
        """
        åŠ è½½è§†è§‰è¯„åˆ†æ•°æ®
        
        Args:
            pic_analyze_dir: pic_analyzeç»„ä»¶çš„ç›®å½•è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        try:
            self.visual_integrator = VisualScoreIntegrator(pic_analyze_dir)
            success = self.visual_integrator.load_visual_scores()
            
            if success:
                self.use_visual_integration = True
                print("âœ… è§†è§‰è¯„åˆ†æ•´åˆå·²å¯ç”¨")
                
                # ç®€åŒ–ç»Ÿè®¡ä¿¡æ¯æ˜¾ç¤ºï¼Œé¿å…æ½œåœ¨çš„ç´¢å¼•é”™è¯¯
                try:
                    if hasattr(self.visual_integrator, 'visual_scores') and self.visual_integrator.visual_scores:
                        method_count = len(self.visual_integrator.visual_scores)
                        print(f"ğŸ“Š æˆåŠŸåŠ è½½ {method_count} ä¸ªæ–¹æ³•çš„è§†è§‰è¯„åˆ†æ•°æ®")
                except Exception as stats_error:
                    print(f"âš ï¸ ç»Ÿè®¡ä¿¡æ¯æ˜¾ç¤ºå¤±è´¥: {stats_error}")
                
                return True
            else:
                self.use_visual_integration = False
                print("âŒ è§†è§‰è¯„åˆ†æ•°æ®åŠ è½½å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ è§†è§‰è¯„åˆ†æ•´åˆåˆå§‹åŒ–å¤±è´¥: {e}")
            self.use_visual_integration = False
            return False

    def process_batch(self, samples_list: List[Dict[str, Any]], return_gpt_response: bool = False,
                     target_domains: Optional[List[int]] = None):
        """
        æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬
        :param samples_list: æ¯ä¸ªå…ƒç´ åŒ…å«assignment_df, deg_dict, composition_dictï¼ˆå¯é€‰ï¼‰
        :param return_gpt_response: æ˜¯å¦è¿”å›gptåŸå§‹response
        :return: (æ ¼å¼åŒ–ç»“æœ, gptåŸå§‹response, gptå¤æŸ¥æ—¥å¿—) æˆ– æ ¼å¼åŒ–ç»“æœ
        """
        all_results = []
        all_gpt_raw_responses = []
        gpt_logs = []
        for i, sample in enumerate(samples_list):
            print(f"\n[Info] æ­£åœ¨å¤„ç†ç¬¬{i+1}ä¸ªæ ·æœ¬ ...")
            result, gpt_raw, gpt_log = self.process_sample(
                sample,
                return_gpt_response=True,
                target_domains=target_domains
            )
            all_results.append(result)
            all_gpt_raw_responses.append(gpt_raw)
            gpt_logs.append(gpt_log)
        # è·¨æ ·æœ¬æ¯”è¾ƒï¼Œé€‰å‡ºæ¯ä¸ªdomainçš„æœ€ä½³åˆ†æ•°
        best_domains = self.cross_sample_compare(all_results)
        formatted = self.format_output(best_domains)
        if return_gpt_response:
            return formatted, all_gpt_raw_responses, gpt_logs
        else:
            return formatted

    def process_sample(self, sample: Dict[str, Any], return_gpt_response: bool = False,
                      target_domains: Optional[List[int]] = None) -> Any:
        """
        å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œéå†æ¯ä¸ªdomainï¼Œç»„ç»‡è¾“å…¥ï¼Œæ‰¹é‡è°ƒç”¨GPTDomainScoreræ‰“åˆ†
        :param sample: åŒ…å«assignment_df, deg_dict, composition_dictï¼ˆå¯é€‰ï¼‰
        :param return_gpt_response: æ˜¯å¦è¿”å›gptåŸå§‹response
        :return: (ç»“æœ, gptåŸå§‹response, gptå¤æŸ¥æ—¥å¿—) æˆ– ç»“æœ
        """
        # åˆ›å»ºæ•°æ®å‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        assignment_df = sample['assignment_df'].copy()
        
        deg_dict = sample['deg_dict']
        deg_df = sample.get('deg_df')
        composition_dict = None
        img_metrics = sample.get('img_metrics', {})
        
        # ç¡®ä¿domain_indexåˆ—ä¸ºæ•´æ•°ç±»å‹
        assignment_df['domain_index'] = assignment_df['domain_index'].astype(int)
        
        # === è®¡ç®—ç©ºé—´åº¦é‡ï¼ˆä½¿ç”¨ spatial.py ä¸­çš„ SpatialMetricsï¼‰ ===
        from spatial import SpatialMetrics  # è½»é‡çº§å¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
        spatial_calc = SpatialMetrics()
        
        # éªŒè¯åæ ‡æ•°æ®
        coord_valid, coord_msg = spatial_calc.validate_coordinates(assignment_df)
        if not coord_valid:
            print(f"[Warning] åæ ‡éªŒè¯è­¦å‘Š: {coord_msg}")
        
        # è®¡ç®—ç©ºé—´åº¦é‡
        spatial_stats = spatial_calc.compute_spatial_stats(assignment_df)
        comp_dict = spatial_stats['compactness']
        adj_dict = spatial_stats['adjacency']
        convexity_dict = spatial_stats.get('convexity', {})
        jaggedness_dict = spatial_stats.get('jaggedness', {})
        conn_metrics_all = spatial_stats.get('connectivity_metrics', {})
        
        # è·å–æ‰€æœ‰domainå¹¶ç¡®ä¿ä¸ºæ•´æ•°ç±»å‹
        domain_indices = sorted(set(assignment_df['domain_index'].astype(int)))
        # å¦‚æœæŒ‡å®šäº† target_domainsï¼Œä»…ä¿ç•™è¿™äº› domain
        if target_domains:
            domain_indices = [d for d in domain_indices if d in target_domains]
        
        # Cache global row/col maxima for normalized position (if available)
        if "array_row" in assignment_df.columns and "array_col" in assignment_df.columns:
            self._row_max = assignment_df["array_row"].max()
            self._col_max = assignment_df["array_col"].max()
        else:
            self._row_max = None
            self._col_max = None

        # é‡ç”¨å·²åˆå§‹åŒ–çš„ExampleStoreå®ä¾‹
        example_store = None
        domains_input = []
        gpt_raw_responses = []
        
        for d in domain_indices:
            # Get spots for this domain (handle missing spot_id column)
            domain_spots = assignment_df[assignment_df['domain_index'] == d]
            if 'spot_id' in domain_spots.columns:
                spots = domain_spots['spot_id'].tolist()
            else:
                spots = domain_spots.index.astype(str).tolist()
            n_spots = len(spots)
            
            # ä½¿ç”¨ç»Ÿä¸€çš„åŸŸæ ‡è¯†ï¼Œä¸å†ä¾èµ–å¤–éƒ¨layer_guess
            layer = f"Domain_{d}"
            
            # ç»Ÿä¸€ä½¿ç”¨å­—ç¬¦ä¸²é”®æŸ¥æ‰¾DEGï¼Œç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§
            degs = deg_dict.get(str(d), [])
            
            # DEG stats if dataframe available
            if deg_df is not None:
                df_dom = deg_df[deg_df['domain'] == d]
                if not df_dom.empty:
                    # ----- NEW: choose top-N DEGs by significance -----
                    degs = self._get_top_degs(df_dom, n=self.top_n_deg)
                    # stats
                    max_logfc = df_dom['logfoldchanges'].max()
                    mean_logfc = df_dom['logfoldchanges'].mean()
                    median_adjP = df_dom['pvals_adj'].median()
                    mean_score = df_dom['scores'].mean()
                else:
                    degs = []
                    max_logfc = mean_logfc = median_adjP = mean_score = np.nan
            else:
                # å¦‚æœæ²¡æœ‰deg_dfï¼Œä½¿ç”¨deg_dictå¹¶é™åˆ¶æ•°é‡
                if not degs:  # å¦‚æœå­—ç¬¦ä¸²é”®æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ•´æ•°é”®
                    degs = deg_dict.get(d, [])
                degs = degs[:self.top_n_deg]
                max_logfc = mean_logfc = median_adjP = mean_score = np.nan
            
            # ç©ºé—´åº¦é‡ï¼šä½¿ç”¨å‰é¢è®¡ç®—ç»“æœ
            compactness = comp_dict.get(d, np.nan)
            adjacency = adj_dict.get(d, np.nan)
            dispersion = spatial_stats['dispersion'].get(d, np.nan)
            
            # è¿é€šæ€§å’Œæœ€è¿‘é‚»è·ç¦»æŒ‡æ ‡
            connectivity_metrics = spatial_stats.get('connectivity_metrics', {}).get(d, {})
            connected_components = connectivity_metrics.get('connected_components', np.nan)
            fragmentation_index = connectivity_metrics.get('fragmentation_index', np.nan)
            
            nn_metrics = spatial_stats.get('nearest_neighbor_distances', {}).get(d, {})
            mean_nn_dist = nn_metrics.get('mean_nn_dist', np.nan)
            median_nn_dist = nn_metrics.get('median_nn_dist', np.nan)
            sub_df = assignment_df[assignment_df["domain_index"] == d]
            
            # è·å–è¯¥domainçš„å›¾ç‰‡è´¨é‡æŒ‡æ ‡
            domain_img_quality = {}
            if img_metrics and 'domain_quality' in img_metrics:
                domain_img_quality = img_metrics['domain_quality'].get(layer, {})

            # --- è·å–Pathwayæ‘˜è¦ (æ–°å¢) ---
            pathway_summary_str = ""
            if self.pathway_data:
                # å°è¯•è·å–å½“å‰æ–¹æ³•çš„pathwayæ•°æ®
                # sampleä¸­å¯èƒ½æœ‰method_nameï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°è¯•ä»self.pathway_dataçš„é”®ä¸­æ¨æ–­ï¼ˆå¦‚æœåªæœ‰ä¸€ä¸ªï¼‰
                method_name = sample.get('method_name')
                
                current_pathway_df = None
                if method_name and method_name in self.pathway_data:
                    current_pathway_df = self.pathway_data[method_name]
                elif len(self.pathway_data) == 1:
                    # å¦‚æœåªåŠ è½½äº†ä¸€ä¸ªæ–¹æ³•çš„æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨
                    current_pathway_df = list(self.pathway_data.values())[0]
                
                if current_pathway_df is not None:
                    pathway_summary_str = self.pathway_analyzer.get_top_pathways_summary(
                        current_pathway_df, d, top_n=3
                    )

            # shape metrics for this domain
            convexity = convexity_dict.get(d, np.nan)
            jaggedness = jaggedness_dict.get(d, np.nan)
            
            summary = self._make_summary(
                d,
                sub_df=sub_df,
                degs=degs,
                compact=compactness,
                adj=adjacency,
                dispersion=dispersion,
                n_spots=n_spots,
                layer=layer,
                comp_dict=None,
                deg_stats={
                    'max_logfc': max_logfc,
                    'mean_logfc': mean_logfc,
                    'median_adjP': median_adjP,
                    'mean_score': mean_score,
                },
                convexity=convexity,
                jaggedness=jaggedness,
                img_quality=domain_img_quality,
                connectivity_stats={
                    'connected_components': connected_components,
                    'fragmentation_index': fragmentation_index,
                },
                nn_stats={
                    'mean_nn_dist': mean_nn_dist,
                    'median_nn_dist': median_nn_dist,
                },
                pathway_summary=pathway_summary_str
            )

            # --- retrieve examples ---
            examples = []
            if example_store is not None:
                examples = example_store.similar_examples(summary, k=self.knn_k)  # may be []

            prompt_parts = [TASK_DESCRIPTION, GLOSSARY_DEFAULT, DEG_GLOSSARY, RUBRIC_TEXT]
            import re
            for idx, ex in enumerate(examples):
                summ = ex['summary']
                # remove compactness numeric values to avoid leakage
                summ = re.sub(r"compactness=[0-9.]+", "compactness=VALUE", summ)
                prompt_parts.append(f"### Example {idx+1}\n<summary>{summ}</summary>\nScore: {ex.get('total_score',0)}")
            prompt_parts.append(
                (
                    "### New domain\n<summary>" + summary + "</summary>\n"
                    "NOTE: Your justification must be **two sentences**.\n"
                    "1) Sentence 1 â€“ concise biological reasoning (mention â‰¥2 DEGs, at least one pathway or cell-type evidence).\n"
                    "2) Sentence 2 â€“ evidence list starting with `basis:` and MUST include spatial metrics (compactness, connected_components, fragmentation_index), marker_hits, and at least one data source.\n"
                    "Return **exactly one** JSON object (no extra text) with schema:\n"
                    "{\n"
                    "  \"score\": <0-1 float>,\n"
                    "  \"justification\": \"<2 sentences>\"\n"
                    "}\n"
                    "Rules: Score must be between 0.0 and 1.0. Provide detailed justification citing database sources."
                )
            )

            full_prompt = "\n\n".join(prompt_parts)

            result_json = self.gpt_scorer.score_with_prompt(full_prompt)
            gpt_raw_responses.append(result_json)

            # è·å–åŸå§‹LLMè¯„åˆ†
            original_score = result_json.get('score', result_json.get('total', 0))

            # --- Programmatic shape/fragmentation enforcement BEFORE visual integration ---
            # Gather metrics
            conn_metrics = conn_metrics_all.get(d, {})
            connected_components = conn_metrics.get('connected_components', np.nan)
            fragmentation_index = conn_metrics.get('fragmentation_index', np.nan)
            jaggedness = jaggedness_dict.get(d, np.nan)
            convexity = convexity_dict.get(d, np.nan)
            compactness_val = comp_dict.get(d, np.nan)

            rule_deduction = 0.0
            
            # === WHITE MATTER EXEMPTION CHECK ===
            # Check if this domain is oligodendrocyte/white matter - exempt from shape penalties
            is_white_matter = self._is_white_matter_domain(degs, deg_df, d)
            
            if is_white_matter:
                # White matter domains are EXEMPT from jaggedness and over-regularization penalties
                # (white matter boundaries are naturally irregular; white matter can be compact)
                pass  # No shape deductions for white matter
            else:
                # 1) Jaggedness deductions (no caps) - only for non-white-matter domains
                if isinstance(jaggedness, (int, float)) and np.isfinite(jaggedness):
                    if 0.35 <= jaggedness <= 0.45:
                        # linear 0.12..0.20 across 0.35..0.45
                        t = (jaggedness - 0.35) / (0.10 + 1e-9)
                        rule_deduction += 0.12 + 0.08 * float(np.clip(t, 0.0, 1.0))
                    elif jaggedness > 0.45:
                        if jaggedness <= 1.0:
                            # 0.20..0.30 for 0.45..1.0
                            t = (jaggedness - 0.45) / (0.55 + 1e-9)
                            rule_deduction += 0.20 + 0.10 * float(np.clip(t, 0.0, 1.0))
                        elif jaggedness > 3.0:
                            # extreme
                            rule_deduction += 0.30 + 0.30  # base heavy + extra extreme
                        else:
                            # >1.0 and <=3.0
                            rule_deduction += 0.30 + 0.15

                # 3) Over-regularization: compactnessâ‰¥0.90 and convexityâ‰¥0.65 â†’ severe deduction (no cap)
                # Only for non-white-matter domains
                if (isinstance(compactness_val, (int, float)) and np.isfinite(compactness_val) and compactness_val >= 0.90 and
                    isinstance(convexity, (int, float)) and np.isfinite(convexity) and convexity >= 0.65):
                    rule_deduction += 0.30

            # 2) Connected components deductions (no caps) - applies to ALL domains
            if isinstance(connected_components, (int, float)) and np.isfinite(connected_components):
                if connected_components > 50:
                    rule_deduction += 0.40
                elif connected_components > 15:
                    rule_deduction += 0.20

            # Apply rule deduction pre-visual
            rule_adjusted = float(np.clip(original_score - rule_deduction, 0.0, 1.0))
            
            # åº”ç”¨è§†è§‰è¯„åˆ†è°ƒæ•´ï¼ˆå¦‚æœå¯ç”¨äº†è§†è§‰æ•´åˆï¼‰
            final_score = rule_adjusted
            visual_adjustment_info = {}
            
            if self.use_visual_integration and hasattr(self, 'visual_integrator') and self.visual_integrator:
                try:
                    # è·å–æ–¹æ³•åç§°
                    method_name = sample.get('method_name', 'unknown')
                    
                    # åº”ç”¨è§†è§‰è¯„åˆ†è°ƒæ•´
                    adjusted_score, adjustment_info = self.visual_integrator.apply_visual_adjustment(
                        original_score=rule_adjusted,
                        method_name=method_name,
                        domain_id=d
                    )
                    
                    final_score = adjusted_score
                    visual_adjustment_info = adjustment_info
                    
                    print(f"[Visual] {method_name} Domain{d}: {rule_adjusted:.3f} -> {final_score:.3f} (è°ƒæ•´: {adjustment_info.get('adjustment_factor', 1.0):.3f})")
                    
                except Exception as visual_error:
                    print(f"[Warning] è§†è§‰è¯„åˆ†è°ƒæ•´å¤±è´¥ {method_name} Domain{d}: {visual_error}")
                    final_score = rule_adjusted
            
            domains_input.append({
                'domain_index': d,
                'score': final_score,
                'original_score': original_score,
                'shape_rule_deduction': round(rule_deduction, 6),
                'metrics': {
                    'compactness': compactness_val,
                    'connected_components': connected_components,
                    'fragmentation_index': fragmentation_index,
                    'convexity': convexity,
                    'jaggedness': jaggedness
                },
                'visual_adjustment_info': visual_adjustment_info,
                'justification': result_json.get('justification', '')
            })

        domains_result = domains_input
        result = {
            'sample_id': sample.get('sample_id', ''),
            'domains': domains_result
        }
        if return_gpt_response:
            return result, gpt_raw_responses, ""
        else:
            return result

    def cross_sample_compare(self, all_results: List[Dict]) -> List[Dict]:
        """
        è·¨æ ·æœ¬ç¨³å¥èšåˆï¼šå¯¹æ¯ä¸ªdomainæŒ‰ä¸­ä½æ•°é€‰æ‹©ä»£è¡¨ç»“æœï¼ˆé¿å…â€œå–æœ€å¤§â€åå·®ï¼‰ã€‚
        :param all_results: æ‰€æœ‰æ ·æœ¬çš„ç»“æœ
        :return: æ¯ä¸ªdomainçš„ä»£è¡¨ç»“æœ
        """
        # èšåˆåŒä¸€domainçš„æ‰€æœ‰ç»“æœ
        idx_to_list: Dict[int, List[Dict]] = {}
        for res in all_results:
            for d in res['domains']:
                idx = int(d['domain_index'])
                idx_to_list.setdefault(idx, []).append(d)
        # é€‰æ‹©æœ€æ¥è¿‘ä¸­ä½æ•°çš„ç»“æœä½œä¸ºä»£è¡¨
        best_by_median: List[Dict] = []
        for idx in sorted(idx_to_list.keys()):
            lst = idx_to_list[idx]
            scores = np.array([x.get('score', 0.0) for x in lst], dtype=float)
            if len(scores) == 0:
                continue
            med = float(np.median(scores))
            # é€‰å–ä¸ä¸­ä½æ•°æœ€æ¥è¿‘çš„ä¸€ä¸ªä½œä¸ºä»£è¡¨
            i = int(np.argmin(np.abs(scores - med)))
            rep = dict(lst[i])
            # é™„åŠ èšåˆä¿¡æ¯ï¼ˆå¯é€‰ï¼Œä¸å½±å“ä¸‹æ¸¸ï¼‰
            rep['aggregation_info'] = {
                'method': 'median_selector',
                'median_score': round(med, 6),
                'num_samples': len(scores),
                'min_score': round(float(np.min(scores)), 6),
                'max_score': round(float(np.max(scores)), 6)
            }
            best_by_median.append(rep)
        return best_by_median

    def format_output(self, best_domains: List[Dict]) -> Any:
        """
        æ ¹æ®ç”¨æˆ·æŒ‡å®šæ ¼å¼è¾“å‡ºç»“æœ
        :param best_domains: æœ€ä½³domainç»“æœ
        :return: json/dataframe/sqlç­‰
        """
        if self.output_format == 'json':
            return {"best_domains": best_domains}
        elif self.output_format == 'dataframe':
            return pd.DataFrame(best_domains)
        elif self.output_format == 'sql':
            # TODO: å®ç°å†™å…¥SQL/NoSQL
            pass
        else:
            raise ValueError('Unsupported output format')

    # Predefined cell-type marker dictionary (enhanced with neuronal subtypes for DLPFC)
    # References: Allen Brain Atlas, PanglaoDB, CellMarker, Human Cell Atlas
    MARKER_DICT = {
        # Glial cells
        "oligodendrocyte": {"MBP", "PLP1", "MOBP", "MOG", "MAG", "CNP", "CLDN11", "OLIG1", "OLIG2", "SOX10"},
        "OPC": {"PDGFRA", "CSPG4", "VCAN", "GPR17"},  # Oligodendrocyte precursor cells
        "astrocyte": {"GFAP", "AQP4", "SLC1A2", "SLC1A3", "ALDH1L1", "S100B", "GJA1"},
        "microglia": {"CX3CR1", "P2RY12", "TREM2", "TMEM119", "AIF1", "CSF1R"},
        
        # Neuronal subtypes (critical for DLPFC layer identification)
        "excitatory_neuron": {"SLC17A7", "CAMK2A", "GRIN1", "GRIN2B", "NRGN", "SATB2"},
        "inhibitory_neuron": {"GAD1", "GAD2", "SLC32A1"},
        "inhibitory_PV": {"PVALB"},  # Fast-spiking interneurons
        "inhibitory_SST": {"SST", "NPY"},  # Somatostatin interneurons
        "inhibitory_VIP": {"VIP", "CALB2"},  # VIP interneurons
        
        # Layer-specific neuronal markers for DLPFC
        "pyramidal_L2_L3": {"CUX1", "CUX2", "SATB2", "RASGRF2"},  # Upper layer pyramidal
        "pyramidal_L4": {"RORB", "RSPO1"},  # Granular layer (L4)
        "pyramidal_L5": {"BCL11B", "FEZF2", "CRYM", "TOX"},  # Corticospinal projection neurons
        "pyramidal_L6": {"TLE4", "FOXP2", "SEMA3E"},  # Corticothalamic neurons
        
        # General synaptic markers (pan-neuronal, Tier 2)
        "synaptic": {"SNAP25", "SYN1", "SYT1", "VAMP2", "STX1A", "RBFOX3"},
        
        # Vascular cells
        "endothelial": {"PECAM1", "VWF", "CLDN5", "FLT1", "KDR", "CDH5"},
        "pericyte": {"PDGFRB", "RGS5", "ABCC9", "KCNJ8"},
    }

    # House-keeping gene setï¼ˆexpanded; GTEx/HRT-Atlas/common qPCR controlsï¼‰
    HOUSEKEEPING_GENES = {
        # canonical
        "ACTB", "ACTG1", "B2M", "GAPDH", "HPRT1", "TBP", "RPLP0", "RPL13A", "YWHAZ",
        # protein folding/translation & core machinery
        "EEF1A1", "EEF2", "GNB2L1", "RACK1", "EIF4A2", "NONO",
        # cytoskeleton/tubulins
        "TUBB", "TUBA1A", "TUBB2A",
        # glycolysis/energy
        "PGK1", "ALDOA", "ENO1", "TPI1", "GPI", "LDHA", "PKM", "SDHA",
        # proteasome/ubiquitin
        "PSMB2", "PSMB3", "UBC", "UBB",
        # membrane/transport
        "TFRC", "RPSA",
        # qPCR references
        "GUSB", "HMBS",
        # heat shock/chaperones (treat as non-specific background)
        "HSP90AA1", "HSPA8", "HSPB1", "DNAJB1",
        # selected ribosomal (prefix RPL*/RPS* already filtered; include a few common)
        "RPS18", "RPS27A", "RPL30"
    }

    # Heat-shock/chaperone gene prefixes used to flag negative evidence
    HEAT_SHOCK_PREFIXES = ("HSP", "HSPA", "HSPB", "HSPD", "HSPE", "DNAJ", "DNAJB", "DNAJC")
    HEAT_SHOCK_GENES = {"HSP90AA1", "HSPA8", "HSPB1", "DNAJB1"}

    def _match_markers(self, genes: List[str]) -> List[str]:
        gset = {g.upper() for g in genes}
        hits = [ct for ct, markers in self.MARKER_DICT.items() if gset & markers]
        return hits

    def _count_housekeeping(self, genes: List[str]) -> int:
        """Return # of housekeeping genes present in given gene list"""
        gset = {g.upper() for g in genes}
        return len(gset & self.HOUSEKEEPING_GENES)

    def _count_heatshock(self, genes: List[str]) -> int:
        """Return # of heat-shock/chaperone genes present (negative evidence)."""
        gset = {g.upper() for g in genes}
        heat = {
            "HSP90AA1", "HSP90AB1", "HSP90B1", "HSPA1A", "HSPA1B", "HSPA5", "HSPA8",
            "HSPH1", "HSPD1", "HSPE1", "HSPB1", "HSPB8", "DNAJB1", "DNAJB4"
        }
        return len(gset & heat)

    def _is_white_matter_domain(self, degs: List[str], deg_df: "pd.DataFrame" = None, domain_id: int = None) -> bool:
        """
        Check if domain is white matter (oligodendrocyte-enriched).
        White matter domains are EXEMPT from jaggedness and over-regularization penalties.
        
        Criteria: >=2 oligodendrocyte markers (MBP, PLP1, MOBP, CNP, MOG, MAG) in top DEGs
        """
        OLIGO_MARKERS = {"MBP", "PLP1", "MOBP", "CNP", "MOG", "MAG", "CLDN11", "OLIG1", "OLIG2"}
        
        gset = {g.upper() for g in degs}
        oligo_hits = len(gset & OLIGO_MARKERS)
        
        # If we have DEG dataframe, check expression levels for stronger validation
        if oligo_hits >= 2:
            return True
        
        # Also check if top DEGs by logFC are oligodendrocyte markers
        if deg_df is not None and domain_id is not None:
            df_dom = deg_df[deg_df['domain'] == domain_id]
            if not df_dom.empty:
                top_by_logfc = df_dom.nlargest(3, 'logfoldchanges')['names'].str.upper().tolist()
                top_oligo_hits = len(set(top_by_logfc) & OLIGO_MARKERS)
                if top_oligo_hits >= 1 and oligo_hits >= 1:
                    return True
        
        return False

    def _make_summary(
        self,
        domain_idx: int,
        sub_df: "pd.DataFrame",
        degs: List[str],
        compact: float,
        adj: float,
        dispersion: float,
        n_spots: int,
        layer: str,
        comp_dict,
        deg_stats: dict,
        convexity: float = np.nan,
        jaggedness: float = np.nan,
        img_quality: dict = None,
        connectivity_stats: dict = None,
        nn_stats: dict = None,
        pathway_summary: str = None,
    ):
        # show up to 5 genes for richer context
        top_genes = degs[:5]
        gene_part = ", ".join(top_genes) + (" â€¦" if len(degs) > len(top_genes) else "")

        # --- position & embedding features ---
        # Check if array_row and array_col exist
        if (self._row_max is not None and self._col_max is not None and 
            "array_row" in sub_df.columns and "array_col" in sub_df.columns):
            row_norm = sub_df["array_row"].mean() / self._row_max
            col_norm = sub_df["array_col"].mean() / self._col_max
        else:
            row_norm = None
            col_norm = None

        # expression summary (handle datasets that lack these columns)
        mean_counts = sub_df["total_counts"].mean() if "total_counts" in sub_df.columns else np.nan
        mean_genes = sub_df["n_genes"].mean() if "n_genes" in sub_df.columns else np.nan
        mean_hv = sub_df["hv_mean"].mean() if "hv_mean" in sub_df.columns else np.nan

        # embedding features (check if available)
        pca1 = sub_df["embedding_pca1"].mean() if "embedding_pca1" in sub_df.columns else np.nan
        pca2 = sub_df["embedding_pca2"].mean() if "embedding_pca2" in sub_df.columns else np.nan

        marker_hits_list = self._match_markers(top_genes)
        marker_hits = ",".join(marker_hits_list) if marker_hits_list else "NA"

        hk_hits = self._count_housekeeping(top_genes)
        hs_hits = self._count_heatshock(top_genes)

        # DEG stats string
        deg_stat_str = (
            f"deg_stats: maxLogFC={deg_stats['max_logfc']:.1f} "
            f"meanLogFC={deg_stats['mean_logfc']:.1f} "
            f"medianAdjP={deg_stats['median_adjP']:.1e} "
            f"meanScore={deg_stats['mean_score']:.1f}; "
            if not np.isnan(deg_stats['max_logfc']) else ""
        )

        # Build summary string with only available data
        summary_parts = []
        
        # Basic domain info
        basic_info = f"Domain {domain_idx} (layer={layer}, spots={n_spots}"
        if row_norm is not None and col_norm is not None:
            basic_info += f", pos=({row_norm:.2f},{col_norm:.2f})"
        basic_info += ")"
        summary_parts.append(basic_info)
        
        # DEGs info
        if gene_part:
            summary_parts.append(f"DEGs: {gene_part}")
        
        # Pathway info (NEW)
        if pathway_summary and pathway_summary != "No pathways available":
            summary_parts.append(f"Pathways: {pathway_summary}")
        
        # Spatial metrics (adjacency, Moran's I, and Geary's C removed from scoring but still computed for analysis)
        spatial_parts = []
        if not np.isnan(compact):
            spatial_parts.append(f"compactness={compact:.2f}")
        if not np.isnan(dispersion):
            spatial_parts.append(f"dispersion={dispersion:.2f}")
        
        # è¿é€šæ€§æŒ‡æ ‡
        if connectivity_stats:
            connected_comp = connectivity_stats.get('connected_components', np.nan)
            frag_index = connectivity_stats.get('fragmentation_index', np.nan)
            
            if not np.isnan(connected_comp):
                spatial_parts.append(f"connected_components={int(connected_comp)}")
            if not np.isnan(frag_index):
                spatial_parts.append(f"fragmentation_index={frag_index:.3f}")
        
        # æœ€è¿‘é‚»è·ç¦»æŒ‡æ ‡
        if nn_stats:
            mean_nn = nn_stats.get('mean_nn_dist', np.nan)
            median_nn = nn_stats.get('median_nn_dist', np.nan)
            
            if not np.isnan(mean_nn):
                spatial_parts.append(f"mean_nn_dist={mean_nn:.3f}")
            if not np.isnan(median_nn):
                spatial_parts.append(f"median_nn_dist={median_nn:.3f}")
        
        if spatial_parts:
            summary_parts.append(f"spatial metrics: {' '.join(spatial_parts)}")
        
        # Embedding info (if available)
        if not np.isnan(pca1) and not np.isnan(pca2):
            summary_parts.append(f"embedding: ({pca1:.2f},{pca2:.2f})")
        
        # Expression summary (only non-empty values)
        expr_parts = []
        if not np.isnan(mean_counts):
            expr_parts.append(f"counts={mean_counts:.0f}")
        if not np.isnan(mean_genes):
            expr_parts.append(f"genes={mean_genes:.0f}")
        if not np.isnan(mean_hv):
            expr_parts.append(f"hv_mean={mean_hv:.2f}")
        if expr_parts:
            summary_parts.append(f"expr_summary: {' '.join(expr_parts)}")
        
        # DEG stats (if available)
        if deg_stat_str:
            summary_parts.append(deg_stat_str.strip())
        
        # Marker & housekeeping hits
        summary_parts.append(f"marker_hits: {marker_hits}")
        summary_parts.append(f"housekeeping_hits: {hk_hits}/{len(top_genes)}")
        summary_parts.append(f"heatshock_hits: {hs_hits}/{len(top_genes)} (negative evidence)")

        # Shape metrics
        if not np.isnan(convexity) or not np.isnan(jaggedness):
            cv = f"{convexity:.3f}" if not np.isnan(convexity) else "nan"
            jg = f"{jaggedness:.3f}" if not np.isnan(jaggedness) else "nan"
            summary_parts.append(f"shape: convexity={cv} jaggedness={jg}")
        
        # å›¾ç‰‡è´¨é‡æŒ‡æ ‡ (åŸºäºæ ‡å‡†é¢œè‰²åˆ†æ)
        if img_quality and isinstance(img_quality, dict):
            img_parts = []
            if 'color_accuracy' in img_quality:
                img_parts.append(f"color_accuracy={img_quality['color_accuracy']:.3f}")
            if 'color_consistency' in img_quality:
                img_parts.append(f"color_consistency={img_quality['color_consistency']:.3f}")
            if 'spatial_coherence' in img_quality:
                img_parts.append(f"spatial_coherence={img_quality['spatial_coherence']:.3f}")
            if 'boundary_clarity' in img_quality:
                img_parts.append(f"boundary_clarity={img_quality['boundary_clarity']:.3f}")
            if 'cross_method_score' in img_quality:
                img_parts.append(f"cross_method_score={img_quality['cross_method_score']:.3f}")
            if 'overall_quality' in img_quality:
                img_parts.append(f"img_overall={img_quality['overall_quality']:.3f}")
            if img_parts:
                summary_parts.append(f"image_quality: {' '.join(img_parts)}")
        
        return " | ".join(summary_parts) + "; "

    # Biologically relevant mitochondrial genes (not pure housekeeping)
    # These are important in neurodegeneration and metabolic studies
    MITO_RELEVANT_GENES = {
        "MT-ND1", "MT-ND2", "MT-ND3", "MT-ND4", "MT-ND5", "MT-ND6",  # Complex I - NADH dehydrogenase
        "MT-CO1", "MT-CO2", "MT-CO3",  # Complex IV - Cytochrome c oxidase
        "MT-ATP6", "MT-ATP8",  # Complex V - ATP synthase
        "MT-CYB",  # Complex III - Cytochrome b
    }
    
    def _get_top_degs(self, df_dom: "pd.DataFrame", n: int = 5) -> List[str]:
        """Return the top-n DEGs using a robust ranking with weighted filtering.

        Enhanced strategy (biologically informed):
        - Mitochondrial genes (MT-*): Downweight by 0.5x instead of removing
          (important in neurodegeneration studies)
        - Ribosomal genes (RPL*/RPS*): Downweight by 0.3x (mostly technical noise)
        - Housekeeping genes: Downweight by 0.2x (non-specific)
        - Robust rank: rank_score = weight * clip(|logFC|, 0.1, 50) * clip(-log10(padj), 1, 100)
        - Falls back to original ordering if required columns missing.
        """
        required_cols = {"logfoldchanges", "pvals_adj", "names"}
        if not required_cols.issubset(df_dom.columns):
            return df_dom["names"].head(n).tolist()

        df = df_dom.copy()
        # Clean and prepare columns
        df["names"] = df["names"].astype(str)
        names_upper = df["names"].str.upper()
        
        # Replace Â±inf with NaN, then compute abs and clip
        logfc = df["logfoldchanges"].replace([np.inf, -np.inf], np.nan)
        padj = df["pvals_adj"].replace(0, np.nan).fillna(1e-300).clip(lower=1e-300)
        logfc_abs = logfc.abs().clip(lower=0.1, upper=50)
        neglogp = (-np.log10(padj)).clip(lower=1, upper=100)
        
        # Base score
        base_score = logfc_abs * neglogp
        
        # Apply biologically-informed weights instead of hard filtering
        # Initialize weights to 1.0
        weights = pd.Series(1.0, index=df.index)
        
        # Mitochondrial genes: downweight but don't remove (relevant in neurodegeneration)
        is_mito = names_upper.str.startswith("MT-")
        # Further distinguish: biologically relevant MT genes get 0.7x, others 0.5x
        is_mito_relevant = names_upper.isin(self.MITO_RELEVANT_GENES)
        weights.loc[is_mito & is_mito_relevant] = 0.7  # Relevant MT genes
        weights.loc[is_mito & ~is_mito_relevant] = 0.5  # Other MT genes
        
        # Ribosomal genes: strong downweight (mostly technical noise)
        is_ribo = names_upper.str.startswith("RPL") | names_upper.str.startswith("RPS")
        weights.loc[is_ribo] = 0.3
        
        # Housekeeping genes: strongest downweight
        is_house = names_upper.isin({g.upper() for g in self.HOUSEKEEPING_GENES})
        weights.loc[is_house] = 0.2
        
        # Compute weighted robust rank
        df["_robust_rank"] = base_score * weights
        
        # Sort by weighted robust rank and pick top-n
        top = df.sort_values("_robust_rank", ascending=False).head(n)
        genes = top["names"].tolist()
        return genes

# ç¤ºä¾‹ç”¨æ³•
if __name__ == '__main__':
    # è¿™é‡Œå¯ä»¥æ·»åŠ å‘½ä»¤è¡Œå‚æ•°è§£æï¼Œè¯»å–è¾“å…¥æ–‡ä»¶ç­‰
    pass 
