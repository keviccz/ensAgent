#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è§†è§‰è¯„åˆ†æ•´åˆå™¨
å°†pic_analyzeç»„ä»¶çš„è§†è§‰è¯„åˆ†æ•´åˆåˆ°ä¸»è¯„åˆ†ç³»ç»Ÿä¸­
"""

import json
import os
import numpy as np
from typing import Dict, Any, Optional, Tuple
from pathlib import Path


class VisualScoreIntegrator:
    """è§†è§‰è¯„åˆ†æ•´åˆå™¨ï¼Œè´Ÿè´£åŠ è½½å’Œå¤„ç†pic_analyzeçš„è§†è§‰è¯„åˆ†æ•°æ®"""
    
    def __init__(self, pic_analyze_dir: str = "pic_analyze"):
        """
        åˆå§‹åŒ–è§†è§‰è¯„åˆ†æ•´åˆå™¨
        
        Args:
            pic_analyze_dir: pic_analyzeç»„ä»¶çš„ç›®å½•è·¯å¾„
        """
        self.pic_analyze_dir = Path(pic_analyze_dir)
        self.visual_scores = None
        self.integration_config = {
            'visual_weight': 0.35,  # è§†è§‰è¯„åˆ†åœ¨æœ€ç»ˆè¯„åˆ†ä¸­çš„æƒé‡ (ä¸­ç­‰æƒé‡ç¡®ä¿è§†è§‰åˆ†æå½±å“)
            'smoothing_factor': 0.8,  # å¹³æ»‘å› å­ï¼Œé¿å…è¿‡åº¦è°ƒæ•´
            'min_adjustment': 0.80,   # æœ€å°è°ƒæ•´å› å­ (è°ƒæ•´èŒƒå›´æ‰©å¤§)
            'max_adjustment': 1.25,   # æœ€å¤§è°ƒæ•´å› å­ (è°ƒæ•´èŒƒå›´æ‰©å¤§)
        }
        
    def load_visual_scores(self) -> bool:
        """
        åŠ è½½pic_analyzeçš„è§†è§‰è¯„åˆ†æ•°æ®
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        try:
            scores_file = self.pic_analyze_dir / "output" / "all_domains_scores.json"
            
            if not scores_file.exists():
                print(f"âš ï¸ è§†è§‰è¯„åˆ†æ–‡ä»¶ä¸å­˜åœ¨: {scores_file}")
                return False
            
            with open(scores_file, 'r', encoding='utf-8') as f:
                self.visual_scores = json.load(f)
            
            print(f"âœ… æˆåŠŸåŠ è½½è§†è§‰è¯„åˆ†æ•°æ®ï¼ŒåŒ…å« {len(self.visual_scores)} ä¸ªæ–¹æ³•")
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            self._validate_visual_scores()
            
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½è§†è§‰è¯„åˆ†å¤±è´¥: {e}")
            return False
    
    def _validate_visual_scores(self):
        """éªŒè¯è§†è§‰è¯„åˆ†æ•°æ®çš„å®Œæ•´æ€§"""
        if not self.visual_scores:
            return
        
        expected_domains = set(range(1, 8))  # domain1-7
        
        for method, scores in self.visual_scores.items():
            # æ£€æŸ¥domainå®Œæ•´æ€§
            actual_domains = set(int(k.replace('domain', '')) for k in scores.keys())
            missing_domains = expected_domains - actual_domains
            
            if missing_domains:
                print(f"âš ï¸ æ–¹æ³• {method} ç¼ºå°‘domainè¯„åˆ†: {missing_domains}")
            
            # æ£€æŸ¥è¯„åˆ†èŒƒå›´
            for domain, score in scores.items():
                if not (0 <= score <= 1):
                    print(f"âš ï¸ æ–¹æ³• {method} domain {domain} è¯„åˆ†è¶…å‡ºèŒƒå›´: {score}")
    
    def get_visual_adjustment_factor(self, method_name: str, domain_id: int) -> float:
        """
        è·å–æŒ‡å®šæ–¹æ³•å’Œdomainçš„è§†è§‰è°ƒæ•´å› å­
        
        Args:
            method_name: æ–¹æ³•åç§°
            domain_id: domain ID (1-7)
            
        Returns:
            float: è°ƒæ•´å› å­ (0.85-1.15)
        """
        if not self.visual_scores or method_name not in self.visual_scores:
            return 1.0  # é»˜è®¤æ— è°ƒæ•´
        
        domain_key = f"domain{domain_id}"
        if domain_key not in self.visual_scores[method_name]:
            return 1.0
        
        visual_score = self.visual_scores[method_name][domain_key]
        
        # è®¡ç®—ç›¸å¯¹äºæ‰€æœ‰æ–¹æ³•åœ¨è¯¥domainä¸Šçš„è¡¨ç°
        all_domain_scores = []
        for method_scores in self.visual_scores.values():
            if domain_key in method_scores:
                all_domain_scores.append(method_scores[domain_key])
        
        if not all_domain_scores:
            return 1.0
        
        # è®¡ç®—ç›¸å¯¹æ’å (0-1)
        relative_rank = self._calculate_relative_rank(visual_score, all_domain_scores)
        
        # è½¬æ¢ä¸ºè°ƒæ•´å› å­
        adjustment_factor = self._rank_to_adjustment_factor(relative_rank)
        
        return adjustment_factor
    
    def _calculate_relative_rank(self, score: float, all_scores: list) -> float:
        """
        è®¡ç®—ç›¸å¯¹æ’å
        
        Args:
            score: å½“å‰åˆ†æ•°
            all_scores: æ‰€æœ‰åˆ†æ•°åˆ—è¡¨
            
        Returns:
            float: ç›¸å¯¹æ’å (0-1)
        """
        if len(all_scores) <= 1:
            return 0.5
        
        # è®¡ç®—æœ‰å¤šå°‘åˆ†æ•°ä½äºå½“å‰åˆ†æ•°
        lower_count = sum(1 for s in all_scores if s < score)
        
        # ç›¸å¯¹æ’å
        relative_rank = lower_count / (len(all_scores) - 1)
        
        return relative_rank
    
    def _rank_to_adjustment_factor(self, relative_rank: float) -> float:
        """
        å°†ç›¸å¯¹æ’åè½¬æ¢ä¸ºè°ƒæ•´å› å­
        
        Args:
            relative_rank: ç›¸å¯¹æ’å (0-1)
            
        Returns:
            float: è°ƒæ•´å› å­
        """
        # ä½¿ç”¨å¹³æ»‘çš„sigmoidå‡½æ•°é¿å…æç«¯è°ƒæ•´
        smoothing = self.integration_config['smoothing_factor']
        
        # å°†æ’åè½¬æ¢ä¸ºè°ƒæ•´å› å­
        # æ’åé«˜çš„æ–¹æ³•è·å¾—æ­£å‘è°ƒæ•´ï¼Œæ’åä½çš„è·å¾—è´Ÿå‘è°ƒæ•´
        normalized_rank = (relative_rank - 0.5) * 2  # è½¬æ¢åˆ° [-1, 1]
        
        # åº”ç”¨å¹³æ»‘å› å­
        smooth_adjustment = normalized_rank * smoothing
        
        # è½¬æ¢ä¸ºè°ƒæ•´å› å­ [min_adjustment, max_adjustment]
        min_adj = self.integration_config['min_adjustment']
        max_adj = self.integration_config['max_adjustment']
        
        if smooth_adjustment >= 0:
            # æ­£å‘è°ƒæ•´
            adjustment_factor = 1.0 + smooth_adjustment * (max_adj - 1.0)
        else:
            # è´Ÿå‘è°ƒæ•´
            adjustment_factor = 1.0 + smooth_adjustment * (1.0 - min_adj)
        
        # ç¡®ä¿åœ¨èŒƒå›´å†…
        adjustment_factor = np.clip(adjustment_factor, min_adj, max_adj)
        
        return adjustment_factor
    
    def get_method_visual_summary(self, method_name: str) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šæ–¹æ³•çš„è§†è§‰è¯„åˆ†æ‘˜è¦
        
        Args:
            method_name: æ–¹æ³•åç§°
            
        Returns:
            dict: è§†è§‰è¯„åˆ†æ‘˜è¦
        """
        if not self.visual_scores or method_name not in self.visual_scores:
            return {}
        
        method_scores = self.visual_scores[method_name]
        scores_list = list(method_scores.values())
        
        summary = {
            'average_visual_score': np.mean(scores_list),
            'min_visual_score': min(scores_list),
            'max_visual_score': max(scores_list),
            'std_visual_score': np.std(scores_list),
            'domain_scores': method_scores.copy()
        }
        
        # è®¡ç®—åœ¨æ‰€æœ‰æ–¹æ³•ä¸­çš„æ’å
        all_averages = []
        for method, scores in self.visual_scores.items():
            avg_score = np.mean(list(scores.values()))
            all_averages.append((method, avg_score))
        
        all_averages.sort(key=lambda x: x[1], reverse=True)
        
        for rank, (method, _) in enumerate(all_averages, 1):
            if method == method_name:
                summary['visual_rank'] = rank
                summary['visual_rank_total'] = len(all_averages)
                break
        
        return summary
    
    def apply_visual_adjustment(self, original_score: float, method_name: str, 
                              domain_id: int, integration_strength: float = None) -> Tuple[float, Dict[str, Any]]:
        """
        åº”ç”¨è§†è§‰è°ƒæ•´åˆ°åŸå§‹è¯„åˆ†
        
        Args:
            original_score: åŸå§‹è¯„åˆ†
            method_name: æ–¹æ³•åç§°
            domain_id: domain ID
            integration_strength: æ•´åˆå¼ºåº¦ (0-1)ï¼ŒNoneä½¿ç”¨é»˜è®¤å€¼
            
        Returns:
            tuple: (è°ƒæ•´åè¯„åˆ†, è°ƒæ•´ä¿¡æ¯)
        """
        if integration_strength is None:
            integration_strength = self.integration_config['visual_weight']
        
        # è·å–è§†è§‰è°ƒæ•´å› å­
        visual_factor = self.get_visual_adjustment_factor(method_name, domain_id)
        
        # åº”ç”¨æ¸è¿›å¼æ•´åˆ
        adjustment_factor = 1.0 + integration_strength * (visual_factor - 1.0)
        
        # è®¡ç®—è°ƒæ•´åçš„è¯„åˆ†
        adjusted_score = original_score * adjustment_factor
        
        # ç¡®ä¿è¯„åˆ†åœ¨åˆç†èŒƒå›´å†…
        adjusted_score = np.clip(adjusted_score, 0.0, 1.0)
        
        # è®°å½•è°ƒæ•´ä¿¡æ¯
        adjustment_info = {
            'original_score': original_score,
            'visual_factor': visual_factor,
            'adjustment_factor': adjustment_factor,
            'adjusted_score': adjusted_score,
            'score_change': adjusted_score - original_score,
            'integration_strength': integration_strength
        }
        
        return adjusted_score, adjustment_info
    
    def get_integration_statistics(self) -> Dict[str, Any]:
        """
        è·å–æ•´åˆç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.visual_scores:
            return {}
        
        stats = {
            'total_methods': len(self.visual_scores),
            'total_domains': 7,
            'config': self.integration_config.copy()
        }
        
        # è®¡ç®—è§†è§‰è¯„åˆ†çš„æ•´ä½“ç»Ÿè®¡
        all_visual_scores = []
        method_averages = []
        
        for method, scores in self.visual_scores.items():
            method_scores = list(scores.values())
            all_visual_scores.extend(method_scores)
            method_averages.append(np.mean(method_scores))
        
        stats.update({
            'visual_score_range': [min(all_visual_scores), max(all_visual_scores)],
            'visual_score_mean': np.mean(all_visual_scores),
            'visual_score_std': np.std(all_visual_scores),
            'method_average_range': [min(method_averages), max(method_averages)],
            'method_discrimination': max(method_averages) - min(method_averages)
        })
        
        return stats
    
    def update_integration_config(self, **kwargs):
        """
        æ›´æ–°æ•´åˆé…ç½®
        
        Args:
            **kwargs: é…ç½®å‚æ•°
        """
        for key, value in kwargs.items():
            if key in self.integration_config:
                self.integration_config[key] = value
                print(f"âœ… æ›´æ–°é…ç½® {key}: {value}")
            else:
                print(f"âš ï¸ æœªçŸ¥é…ç½®å‚æ•°: {key}")


def test_visual_integrator():
    """æµ‹è¯•è§†è§‰è¯„åˆ†æ•´åˆå™¨"""
    print("ğŸ§ª æµ‹è¯•è§†è§‰è¯„åˆ†æ•´åˆå™¨...")
    
    integrator = VisualScoreIntegrator()
    
    # åŠ è½½è§†è§‰è¯„åˆ†
    if not integrator.load_visual_scores():
        print("âŒ æ— æ³•åŠ è½½è§†è§‰è¯„åˆ†æ•°æ®")
        return
    
    # æµ‹è¯•è°ƒæ•´å› å­è®¡ç®—
    test_cases = [
        ('STAGATE', 1),
        ('BayesSpace', 4),
        ('SEDR', 6),
        ('DR-SC', 2)
    ]
    
    print("\nğŸ“Š æµ‹è¯•è°ƒæ•´å› å­è®¡ç®—:")
    for method, domain in test_cases:
        factor = integrator.get_visual_adjustment_factor(method, domain)
        summary = integrator.get_method_visual_summary(method)
        
        print(f"  {method} Domain{domain}: è°ƒæ•´å› å­={factor:.3f}, "
              f"å¹³å‡è§†è§‰è¯„åˆ†={summary.get('average_visual_score', 0):.3f}")
    
    # æµ‹è¯•è¯„åˆ†è°ƒæ•´
    print("\nğŸ”§ æµ‹è¯•è¯„åˆ†è°ƒæ•´:")
    original_score = 0.75
    for method, domain in test_cases:
        adjusted_score, info = integrator.apply_visual_adjustment(
            original_score, method, domain
        )
        
        print(f"  {method} Domain{domain}: {original_score:.3f} â†’ {adjusted_score:.3f} "
              f"(å˜åŒ–: {info['score_change']:+.3f})")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = integrator.get_integration_statistics()
    print(f"\nğŸ“ˆ æ•´åˆç»Ÿè®¡:")
    print(f"  è§†è§‰è¯„åˆ†èŒƒå›´: {stats['visual_score_range'][0]:.3f} - {stats['visual_score_range'][1]:.3f}")
    print(f"  æ–¹æ³•åŒºåˆ†åº¦: {stats['method_discrimination']:.3f}")
    print(f"  è§†è§‰æƒé‡: {stats['config']['visual_weight']}")


if __name__ == "__main__":
    test_visual_integrator()
