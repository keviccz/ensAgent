import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)
import logging
import re
import json
import traceback
logging.getLogger("langchain").setLevel(logging.ERROR)

import os
import pandas as pd
import argparse
from tqdm import tqdm
from domain_evaluator import DomainEvaluator
from input_handler import load_and_validate_inputs
from output_handler import write_output
from logger import Logger
# å¯¼å…¥æ„å»ºçŸ©é˜µçš„åŠŸèƒ½
from typing import List, Dict
from sklearn.preprocessing import MinMaxScaler
from decimal import Decimal, getcontext

# å®‰å…¨åœ°å¯¼å…¥é…ç½®ï¼Œå…¼å®¹æœ‰æ— config.pyæ–‡ä»¶çš„æƒ…å†µ
from config_loader import get_legacy_config
try:
    AZURE_OPENAI_KEY, AZURE_ENDPOINT, AZURE_DEPLOYMENT, AZURE_API_VERSION = get_legacy_config()
except ValueError as e:
    print(f"[Error] Configuration error: {e}")
    print("Please set environment variables or create config.py file.")
    print("See README.md for configuration instructions.")
    exit(1)

# é»˜è®¤APIå‚æ•°
DEFAULT_OPENAI_KEY = AZURE_OPENAI_KEY
DEFAULT_AZURE_ENDPOINT = AZURE_ENDPOINT
DEFAULT_AZURE_DEPLOYMENT = AZURE_DEPLOYMENT
DEFAULT_AZURE_API_VERSION = AZURE_API_VERSION

# è§£æå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description='DomainEvaluator Batch Runner')
parser.add_argument('--output_format', type=str, default='json', choices=['json', 'dataframe', 'excel', 'sql', 'markdown'], help='è¾“å‡ºæ ¼å¼ï¼Œå¯é€‰ï¼šjson, dataframe(csv), excel(xlsx), sql, markdown(md)')
parser.add_argument('--openai_key', type=str, default=DEFAULT_OPENAI_KEY, help='OpenAIæˆ–Azure OpenAIçš„APIå¯†é’¥')
parser.add_argument('--azure_endpoint', type=str, default=DEFAULT_AZURE_ENDPOINT, help='Azure OpenAI endpoint')
parser.add_argument('--azure_deployment', type=str, default=DEFAULT_AZURE_DEPLOYMENT, help='Azure OpenAI deploymentåç§°')
parser.add_argument('--azure_api_version', type=str, default=DEFAULT_AZURE_API_VERSION, help='Azure OpenAI APIç‰ˆæœ¬')
parser.add_argument('--sql_db', type=str, default='output/result.db', help='SQLè¾“å‡ºæ—¶çš„sqliteæ•°æ®åº“æ–‡ä»¶è·¯å¾„')
parser.add_argument('--top_n_deg', type=int, default=5, help='æ¯ä¸ªdomainå–å‰Nä¸ªDEGè¿›è¡Œæ‘˜è¦')

# æ–°å¢çš„LLMå‚æ•°
parser.add_argument('--temperature', type=float, default=0.0, help='å¤§æ¨¡å‹é‡‡æ ·æ¸©åº¦ (0.0-2.0)')
parser.add_argument('--max_completion_tokens', type=int, default=30000, help='æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰')
parser.add_argument('--top_p', type=float, default=1.0, help='æ ¸é‡‡æ ·æ¦‚ç‡ (0.0-1.0)')
parser.add_argument('--frequency_penalty', type=float, default=0.0, help='é¢‘ç‡æƒ©ç½š (-2.0-2.0)')
parser.add_argument('--presence_penalty', type=float, default=0.0, help='å­˜åœ¨æƒ©ç½š (-2.0-2.0)')

# å›¾ç‰‡åˆ†æå‚æ•°
# å›¾ç‰‡å¤„ç†åŠŸèƒ½å·²ç§»é™¤

parser.add_argument('--use_example_db', action='store_true', help='æ˜¯å¦å¯ç”¨å‘é‡æ•°æ®åº“Few-shotæ£€ç´¢')
parser.add_argument('--knn_k', type=int, default=4, help='Few-shotæ£€ç´¢è¿”å›Kä¸ªç¤ºä¾‹')
parser.add_argument('--build_matrices', action='store_true', default=True, help='æ˜¯å¦è‡ªåŠ¨æ„å»ºåˆ†æ•°å’Œæ ‡ç­¾çŸ©é˜µ')
parser.add_argument('--no_build_matrices', action='store_true', help='ç¦ç”¨è‡ªåŠ¨çŸ©é˜µæ„å»º')
parser.add_argument('--enforce_discrimination', action='store_true', help='è®©GPTæ‰“åˆ†å¼ºåˆ¶ä¿æŒè¾ƒå¤§çš„åŒºåˆ†åº¦(é»˜è®¤å…³é—­)')
parser.add_argument('--vlm_off', action='store_true', help='å…³é—­è§†è§‰è¯„åˆ†æ•´åˆï¼ˆVLMï¼‰ï¼Œä»…ä½¿ç”¨æ–‡æœ¬ä¸ç©ºé—´/DEGæŒ‡æ ‡')
# ARIè®¡ç®—åŠŸèƒ½å·²ç§»é™¤
# parser.add_argument('--calculate_ari', action='store_true', default=True, help='æ˜¯å¦è®¡ç®—ARIå¹¶ç”Ÿæˆå¯è§†åŒ–')
# parser.add_argument('--no_calculate_ari', action='store_true', help='ç¦ç”¨ARIè®¡ç®—å’Œå¯è§†åŒ–')

parser.add_argument('--skip_existing', action='store_true', help='è·³è¿‡å·²å­˜åœ¨çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆåŒ…æ‹¬ä¸»ç¨‹åºç»“æœï¼‰')
parser.add_argument('--clean_output', action='store_true', help='è¿è¡Œå‰æ¸…ç†æ‰€æœ‰è¾“å‡ºæ–‡ä»¶')

# I/O overrides & Tool-runner integration (Phase B)
parser.add_argument('--input_dir', type=str, default=None, help='è¾“å…¥ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨è„šæœ¬ç›®å½•ä¸‹çš„ input/ï¼‰')
parser.add_argument('--output_dir', type=str, default=None, help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨è„šæœ¬ç›®å½•ä¸‹çš„ output/ï¼‰')
parser.add_argument('--toolrunner_output_dir', type=str, default=None, help='Tool-runner è¾“å‡ºç›®å½•ï¼ˆåŒ…å« spot/ DEGs/ PATHWAY/ å­ç›®å½•ï¼‰ã€‚æä¾›åå°†è‡ªåŠ¨ stage åˆ° scoring input/')
parser.add_argument('--toolrunner_sample_id', type=str, default=None, help='ç”¨äºä» Tool-runner è¾“å‡ºä¸­è¿‡æ»¤æ–‡ä»¶çš„ sample_idï¼ˆä¾‹å¦‚ "DLPFC_151507"ï¼‰ã€‚ä¸ --toolrunner_output_dir ä¸€èµ·ä½¿ç”¨')
parser.add_argument('--toolrunner_overwrite', action='store_true', help='stage Tool-runner è¾“å‡ºæ—¶è¦†ç›– scoring input/ ä¸­å·²å­˜åœ¨çš„æ–‡ä»¶')

# Annotation åŠŸèƒ½å‚æ•°
parser.add_argument('--annotation_multiagent', action='store_true', help='æ˜¯å¦è¿è¡Œ Multi-Agent Domain Annotation æ¨¡å¼ï¼ˆå«VLM/Peer/Critic/Loop/æ—¥å¿—ï¼‰')
parser.add_argument('--domain', type=str, help='æŒ‡å®šè¦ Annotate çš„ domain ID (é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ "1,3")ï¼Œä»…åœ¨ --annotation_multiagent æ¨¡å¼ä¸‹æœ‰æ•ˆ')
parser.add_argument('--annotation_data_dir', type=str, default=None, help='Multi-Agent Annotation è¾“å…¥ç›®å½•ï¼ˆåŒ…å« BEST_<sample_id>_{spot,DEGs,PATHWAY}.csvï¼‰')
parser.add_argument('--annotation_sample_id', type=str, default=None, help='Multi-Agent Annotation çš„ sample_idï¼ˆç”¨äºå®šä½ BEST_* æ–‡ä»¶ä¸ result.pngï¼‰')

args, unknown_args = parser.parse_known_args()

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œç¡®ä¿è·¯å¾„ç›¸å¯¹äºè„šæœ¬æ–‡ä»¶è€Œä¸æ˜¯å·¥ä½œç›®å½•
script_dir = os.path.dirname(os.path.abspath(__file__))
# Default I/O dirs (can be overridden by CLI)
input_dir = args.input_dir or os.path.join(script_dir, 'input')
output_dir = args.output_dir or os.path.join(script_dir, 'output')
# å›¾ç‰‡å¤„ç†åŠŸèƒ½å·²ç§»é™¤


def stage_toolrunner_outputs_to_scoring_input(
    *,
    tool_output_dir: str,
    scoring_input_dir: str,
    sample_id: str,
    overwrite: bool,
):
    """Stage Tool-runner outputs into scoring input/ directory using expected filenames.

    Tool-runner typically outputs:
      <tool_output_dir>/spot/*_spot.csv
      <tool_output_dir>/DEGs/*_DEGs.csv
      <tool_output_dir>/PATHWAY/*_PATHWAY.csv

    Tool-runner file prefixes are usually like:
      IRIS_domain_DLPFC_151507_spot.csv
    while scoring expects:
      IRIS_DLPFC_151507_spot.csv
    """
    import shutil
    import re

    spot_dir = os.path.join(tool_output_dir, "spot")
    deg_dir = os.path.join(tool_output_dir, "DEGs")
    pathway_dir = os.path.join(tool_output_dir, "PATHWAY")

    if not os.path.exists(spot_dir):
        raise FileNotFoundError(f"Not found: {spot_dir}")
    if not os.path.exists(deg_dir):
        raise FileNotFoundError(f"Not found: {deg_dir}")
    if not os.path.exists(pathway_dir):
        raise FileNotFoundError(f"Not found: {pathway_dir}")

    os.makedirs(scoring_input_dir, exist_ok=True)

    domain_prefix_re = re.compile(r"^(?P<method>[^_]+)_domain_(?P<rest>.+)$")

    def rewrite_prefix(prefix: str) -> str:
        m = domain_prefix_re.match(prefix)
        if not m:
            return prefix
        return f"{m.group('method')}_{m.group('rest')}"

    def copy_group(src_dir: str, suffix: str) -> int:
        n = 0
        for name in os.listdir(src_dir):
            if not name.endswith(f"_{suffix}.csv"):
                continue
            prefix = name[: -(len(f"_{suffix}.csv"))]
            if sample_id and (sample_id not in prefix):
                continue
            new_prefix = rewrite_prefix(prefix)
            src = os.path.join(src_dir, name)
            dst = os.path.join(scoring_input_dir, f"{new_prefix}_{suffix}.csv")
            if os.path.exists(dst) and not overwrite:
                continue
            shutil.copy2(src, dst)
            n += 1
        return n

    n_spot = copy_group(spot_dir, "spot")
    n_deg = copy_group(deg_dir, "DEGs")
    n_pw = copy_group(pathway_dir, "PATHWAY")

    if n_spot == 0:
        raise RuntimeError(f"No spot files staged from {spot_dir} (sample_id filter: {sample_id})")
    if n_deg == 0:
        raise RuntimeError(f"No DEG files staged from {deg_dir} (sample_id filter: {sample_id})")
    if n_pw == 0:
        print(f"[Warning] No PATHWAY files staged from {pathway_dir} (sample_id filter: {sample_id})")

    print(f"[Info] Tool-runner outputs staged into: {scoring_input_dir}")
    print(f"  - spot: {n_spot}")
    print(f"  - DEGs: {n_deg}")
    print(f"  - PATHWAY: {n_pw}")


# Phase B: optionally stage tool-runner outputs into scoring input/ before scoring
if args.toolrunner_output_dir:
    if not args.toolrunner_sample_id:
        print("[Error] --toolrunner_sample_id is required when --toolrunner_output_dir is provided.")
        exit(1)
    try:
        stage_toolrunner_outputs_to_scoring_input(
            tool_output_dir=args.toolrunner_output_dir,
            scoring_input_dir=input_dir,
            sample_id=args.toolrunner_sample_id,
            overwrite=bool(args.toolrunner_overwrite),
        )
    except Exception as e:
        print(f"[Error] Failed to stage Tool-runner outputs: {e}")
        traceback.print_exc()
        exit(1)

# æ¸…ç†è¾“å‡ºæ–‡ä»¶åŠŸèƒ½
def clean_output_files(output_dir: str, log_func):
    """æ¸…ç†è¾“å‡ºæ–‡ä»¶"""
    files_to_clean = []
    
    # ä¸»ç¨‹åºç»“æœæ–‡ä»¶
    if os.path.exists(output_dir):
        for file in os.listdir(output_dir):
            if file.endswith('_result.json') or file.endswith('_result.csv') or file.endswith('_result.xlsx') or file.endswith('_result.md'):
                file_path = os.path.join(output_dir, file)
                files_to_clean.append(file_path)
    
    # çŸ©é˜µæ–‡ä»¶
    consensus_dir = os.path.join(output_dir, 'consensus')
    if os.path.exists(consensus_dir):
        consensus_files = [
            'scores_matrix.csv',
            'labels_matrix.csv'
        ]
        for file in consensus_files:
            file_path = os.path.join(consensus_dir, file)
            if os.path.exists(file_path):
                files_to_clean.append(file_path)
    
    if files_to_clean:
        print(f"[Info] æ­£åœ¨æ¸…ç† {len(files_to_clean)} ä¸ªè¾“å‡ºæ–‡ä»¶...")
        log_func(f"æ­£åœ¨æ¸…ç† {len(files_to_clean)} ä¸ªè¾“å‡ºæ–‡ä»¶...")
        
        for file_path in files_to_clean:
            try:
                os.remove(file_path)
                log_func(f"å·²åˆ é™¤: {file_path}")
            except Exception as e:
                log_func(f"åˆ é™¤å¤±è´¥: {file_path} - {e}")
        
        print(f"[Info] æ¸…ç†å®Œæˆ")
        log_func("è¾“å‡ºæ–‡ä»¶æ¸…ç†å®Œæˆ")
    else:
        print("[Info] æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„è¾“å‡ºæ–‡ä»¶")
        log_func("æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„è¾“å‡ºæ–‡ä»¶")

# è§£æå½¢å¦‚ --domain2 / --method=IRIS çš„å‚æ•°ï¼Œåªå¯¹æŒ‡å®š domain / æ–¹æ³• è¿›è¡ŒGPTæ‰“åˆ†
selected_domains: List[int] = []
selected_methods: List[str] = []
for tok in unknown_args:
    m_dom = re.match(r"--domain(\d+)$", tok)
    m_method = re.match(r"--method=(\w+)$", tok)
    if m_dom:
        try:
            selected_domains.append(int(m_dom.group(1)))
        except ValueError:
            continue
    elif m_method:
        selected_methods.append(m_method.group(1))

if selected_domains:
    TARGET_DOMAINS = sorted(set(selected_domains))
    print(f"[Info] ä»…å¯¹ä»¥ä¸‹ domain è¿›è¡Œ GPT æ‰“åˆ†: {TARGET_DOMAINS}")
else:
    TARGET_DOMAINS = None

if selected_methods:
    TARGET_METHODS = sorted(set(selected_methods))
    print(f"[Info] ä»…å¯¹ä»¥ä¸‹æ–¹æ³•è¿›è¡Œ GPT æ‰“åˆ†: {TARGET_METHODS}")
else:
    TARGET_METHODS = None


def merge_domain_results_incremental(prefix: str,
                                     new_result: Dict,
                                     output_dir: str,
                                     target_domains: List[int] | None):
    """
    å¢é‡åˆå¹¶ç»“æœï¼š
    - å¦‚æœæŒ‡å®šäº† TARGET_DOMAINSï¼Œåˆ™ä»å·²æœ‰çš„ {prefix}_result.json ä¸­è¯»å–æ—§ç»“æœï¼Œ
      åªç”¨ new_result ä¸­å¯¹åº” domain_index çš„æ¡ç›®è¦†ç›–æ—§ç»“æœï¼Œå…¶ä½™ domain ä¿æŒä¸å˜ã€‚
    - å¦‚æœæ²¡æœ‰æ—§æ–‡ä»¶æˆ–æœªæŒ‡å®š target_domainsï¼Œåˆ™ç›´æ¥è¿”å› new_resultã€‚
    """
    if not target_domains:
        return new_result

    result_path = os.path.join(output_dir, f"{prefix}_result.json")
    if not os.path.exists(result_path):
        # æ²¡æœ‰æ—§æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨æ–°ç»“æœ
        return new_result

    try:
        with open(result_path, "r", encoding="utf-8") as f:
            old_result = json.load(f)
    except Exception:
        # æ—§æ–‡ä»¶æŸåæˆ–æ— æ³•è§£æï¼Œé¿å…é˜»å¡æµç¨‹ï¼Œç›´æ¥ä½¿ç”¨æ–°ç»“æœ
        return new_result

    # ç»Ÿä¸€è·å–æ—§/æ–° domain åˆ—è¡¨ï¼ˆå…¼å®¹æ—§ç‰ˆ "domains" å­—æ®µï¼‰
    old_list = old_result.get("best_domains", old_result.get("domains", [])) or []
    new_list = new_result.get("best_domains", new_result.get("domains", [])) or []

    # ä»¥ domain_index ä¸ºé”®æ„å»ºæ˜ å°„
    domain_map: Dict[int, Dict] = {}
    for d in old_list:
        idx = d.get("domain_index")
        if idx is None:
            continue
        try:
            domain_map[int(idx)] = d
        except Exception:
            continue

    # ç”¨æ–°ç»“æœè¦†ç›–å¯¹åº” domain
    for d in new_list:
        idx = d.get("domain_index")
        if idx is None:
            continue
        try:
            domain_map[int(idx)] = d
        except Exception:
            continue

    # åˆå¹¶åæŒ‰ domain_index æ’åº
    merged_list = sorted(
        domain_map.values(),
        key=lambda x: int(x.get("domain_index", 0))
    )

    # åœ¨æ—§ç»“æœç»“æ„ä¸Šæ›¿æ¢ domain åˆ—è¡¨ï¼Œä¿ç•™ sample_id ç­‰å…¶å®ƒå­—æ®µ
    old_result["best_domains"] = merged_list
    return old_result

# æ—¥å¿—å¯¹è±¡
logger = Logger()
log = logger.log

# --- Annotation ç‹¬ç«‹è¿è¡Œæ¨¡å¼ (æ–°å¢) ---
if args.annotation_multiagent:
    print("[Info] Enter Multi-Agent Domain Annotation mode ...")
    log("[Info] Enter Multi-Agent Domain Annotation mode ...")

    # Domain filter (same semantics as --annotation)
    target_domains = None
    if args.domain:
        try:
            target_domains = [int(d.strip()) for d in args.domain.split(',')]
            print(f"[Info] æŒ‡å®šè¿è¡Œ Domains(æ¥è‡ª --domain): {target_domains}")
            log(f"[Info] æŒ‡å®šè¿è¡Œ Domains(æ¥è‡ª --domain): {target_domains}")
        except ValueError:
            print("[Error] --domain å‚æ•°æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºé€—å·åˆ†éš”çš„æ•´æ•° (ä¾‹å¦‚ '1,3')")
            log("[Error] --domain å‚æ•°æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºé€—å·åˆ†éš”çš„æ•´æ•° (ä¾‹å¦‚ '1,3')")
            exit(1)
    elif TARGET_DOMAINS:
        target_domains = TARGET_DOMAINS
        print(f"[Info] æŒ‡å®šè¿è¡Œ Domains(æ¥è‡ª --domainX): {target_domains}")
        log(f"[Info] æŒ‡å®šè¿è¡Œ Domains(æ¥è‡ª --domainX): {target_domains}")

    try:
        from config import load_config
        from annotation.annotation_multiagent.orchestrator import run_annotation_multiagent

        cfg = load_config()
        # Override credentials with CLI args if provided (keeps behavior consistent)
        cfg = cfg.update(
            azure_openai_key=args.openai_key,
            azure_endpoint=args.azure_endpoint,
            azure_deployment=args.azure_deployment,
            azure_api_version=args.azure_api_version,
        )

        # Determine sample_id + data_dir (generalized; keeps legacy fallbacks).
        sample_id = args.annotation_sample_id or args.toolrunner_sample_id
        data_dir = args.annotation_data_dir
        potential_dirs = []
        if data_dir:
            potential_dirs.append(os.path.abspath(data_dir))
        potential_dirs.extend(
            [
                os.path.abspath(os.path.join(os.getcwd(), "../ARI&Picture_DLPFC_151507/output")),  # legacy
                os.path.abspath("ARI&Picture_DLPFC_151507/output"),  # legacy
                output_dir,  # scoring output dir
                os.path.abspath(os.path.join(os.path.dirname(output_dir), "ensemble")),  # optional
                os.path.abspath(os.path.join(os.path.dirname(output_dir), "ensemble_output")),  # optional
            ]
        )

        def _exists_best_bundle(d: str, sid: str) -> bool:
            if not sid:
                return False
            candidates = [
                os.path.join(d, f"BEST_{sid}_DEGs.csv"),
                os.path.join(d, f"BEST_DLPFC_{sid}_DEGs.csv"),  # legacy
            ]
            return any(os.path.exists(p) for p in candidates)

        if not sample_id:
            # try infer from files in potential dirs (BEST_*_spot.csv)
            for d in potential_dirs:
                if not os.path.exists(d):
                    continue
                try:
                    for fn in os.listdir(d):
                        if fn.startswith("BEST_") and fn.endswith("_spot.csv"):
                            # BEST_<sample_id>_spot.csv
                            sample_id = fn[len("BEST_") : -len("_spot.csv")]
                            break
                except Exception:
                    continue
                if sample_id:
                    break

        picked = None
        for d in potential_dirs:
            if os.path.exists(d) and sample_id and _exists_best_bundle(d, sample_id):
                picked = d
                break
        if not picked:
            print(f"[Error] æœªæ‰¾åˆ° Multi-Agent Annotation æ‰€éœ€çš„æ•°æ®æ–‡ä»¶ (BEST spot/DEGs/PATHWAY). æœç´¢è·¯å¾„: {potential_dirs}")
            print(f"[Error] è¯·æä¾› --annotation_data_dir ä¸ --annotation_sample_id (æˆ–å…ˆè¿è¡Œ Phase C ç”Ÿæˆ BEST_* æ–‡ä»¶).")
            log("[Error] æœªæ‰¾åˆ° Multi-Agent Annotation æ‰€éœ€çš„æ•°æ®æ–‡ä»¶ (BEST spot/DEGs/PATHWAY).")
            exit(1)
        data_dir = picked

        print(f"[Info] åŠ è½½æ•°æ®æº: {data_dir}")
        log(f"[Info] åŠ è½½æ•°æ®æº: {data_dir}")

        output_base = os.path.dirname(data_dir)
        annot_out_dir = os.path.join(output_base, "annotation_output")
        run_annotation_multiagent(
            data_dir=data_dir,
            sample_id=str(sample_id),
            target_domains=target_domains,
            output_dir=annot_out_dir,
            config=cfg,
        )

        print(f"[Success] Multi-Agent Annotation finished. Results saved to: {annot_out_dir}")
        log("[Success] Multi-Agent Annotation finished.")
        exit(0)
    except Exception as e:
        print(f"[Error] Multi-Agent Annotation è¿è¡Œå¤±è´¥: {e}")
        log(f"[Error] Multi-Agent Annotation è¿è¡Œå¤±è´¥: {e}")
        traceback.print_exc()
        exit(1)

# å¦‚æœéœ€è¦æ¸…ç†è¾“å‡ºæ–‡ä»¶
if args.clean_output:
    clean_output_files(output_dir, log)

print("[Info] æ­£åœ¨æ ¡éªŒè¾“å…¥æ•°æ®...")
log("å¼€å§‹æ ¡éªŒè¾“å…¥æ•°æ®...")
# åŠ è½½å¹¶æ ¡éªŒæ‰€æœ‰è¾“å…¥æ ·æœ¬ï¼Œè¿”å›æ ·æœ¬åˆ—è¡¨ã€æ ·æœ¬åã€æœ‰æ•ˆæ ·æœ¬å
samples_list, sample_names, valid_samples = load_and_validate_inputs(
    input_dir,
    log_func=log,
    # å›¾ç‰‡å¤„ç†åŠŸèƒ½å·²ç§»é™¤
)

if not samples_list:
    print("[Error] æ²¡æœ‰å¯ç”¨çš„æœ‰æ•ˆæ ·æœ¬ï¼Œç¨‹åºç»ˆæ­¢ã€‚è¯·æŸ¥çœ‹output_logè·å–è¯¦ç»†ä¿¡æ¯ã€‚"); log("æ²¡æœ‰å¯ç”¨çš„æœ‰æ•ˆæ ·æœ¬ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
    exit(1)

print(f"[Info] å…±é€šè¿‡æ ¡éªŒçš„æ ·æœ¬æ•°: {len(samples_list)}\n")
log(f"å…±é€šè¿‡æ ¡éªŒçš„æ ·æœ¬æ•°: {len(samples_list)}")

# åˆå§‹åŒ–DomainEvaluatorï¼Œè´Ÿè´£å¤§æ¨¡å‹æ‰“åˆ†

evaluator = DomainEvaluator(
    openai_api_key=args.openai_key,
    output_format=args.output_format,
    azure_endpoint=args.azure_endpoint,
    azure_deployment=args.azure_deployment,
    azure_api_version=args.azure_api_version,
    temperature=args.temperature,
    max_completion_tokens=args.max_completion_tokens,
    top_p=args.top_p,
    frequency_penalty=args.frequency_penalty,
    presence_penalty=args.presence_penalty,
    top_n_deg=args.top_n_deg,
    use_example_db=args.use_example_db,
    knn_k=args.knn_k,
    enforce_discrimination=args.enforce_discrimination
)

# æ‰¹é‡å¤„ç†ï¼Œå¸¦è¿›åº¦æ¡å’Œå¼‚å¸¸å¤„ç†
max_retries = 2
all_results = []
all_gpt_raw_responses = []
all_gpt_logs = []
output_files = []
# ä»…è®°å½•æˆåŠŸå¤„ç†çš„æ ·æœ¬ä¸ç»“æœé…å¯¹ï¼Œé¿å…ç´¢å¼•é—®é¢˜
successful_results: List[tuple[str, Dict]] = []
print("[Info] æ­£åœ¨æ‰¹é‡å¤„ç†æ ·æœ¬...")
log("å¼€å§‹æ‰¹é‡å¤„ç†æ ·æœ¬...")

# ä¸ºæ¯ä¸ªæ ·æœ¬åŠ è½½pathwayæ•°æ®
print("[Info] æ­£åœ¨åŠ è½½Pathwayæ•°æ®...")
all_methods = set()
for idx, sample in enumerate(samples_list):
    sample_id = valid_samples[idx]
    method_name = sample_id.split('_')[0]  # ä»æ ·æœ¬IDæå–æ–¹æ³•å
    sample['method_name'] = method_name
    all_methods.add(method_name)

# ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ–¹æ³•çš„pathwayæ•°æ®
evaluator.pathway_data = evaluator.pathway_analyzer.load_pathway_data(input_dir)
evaluator.pathway_scores = evaluator.pathway_analyzer.analyze_pathway_enrichment(evaluator.pathway_data)
print(f"[Info] æˆåŠŸåŠ è½½ {len(evaluator.pathway_data)} ä¸ªæ–¹æ³•çš„pathwayæ•°æ®: {list(evaluator.pathway_data.keys())}")

# åŠ è½½è§†è§‰è¯„åˆ†æ•°æ®ï¼ˆå¯é€šè¿‡ --vlm_off å…³é—­ï¼‰
if args.vlm_off:
    evaluator.use_visual_integration = False
    print(f"[Info] å·²æŒ‰ --vlm_off å…³é—­è§†è§‰è¯„åˆ†æ•´åˆï¼Œä½¿ç”¨ä¼ ç»Ÿè¯„åˆ†æ–¹å¼")
else:
    print(f"[Info] å°è¯•åŠ è½½è§†è§‰è¯„åˆ†æ•°æ®...")
    visual_loaded = evaluator.load_visual_scores("pic_analyze")
    if visual_loaded:
        print(f"[Info] è§†è§‰è¯„åˆ†æ•´åˆå·²å¯ç”¨ï¼Œå°†å½±å“æœ€ç»ˆè¯„åˆ†")
    else:
        print(f"[Info] è§†è§‰è¯„åˆ†æ•´åˆæœªå¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿè¯„åˆ†æ–¹å¼")

try:
    for idx, sample in enumerate(tqdm(samples_list, desc="Sample Batch")):
        # æ–¹æ³•è¿‡æ»¤ï¼šå¦‚æœæŒ‡å®šäº† TARGET_METHODSï¼Œåˆ™åªå¤„ç†è¿™äº›æ–¹æ³•
        method_name = sample.get('method_name') or valid_samples[idx].split('_')[0]
        if TARGET_METHODS and method_name not in TARGET_METHODS:
            print(f"[Info] è·³è¿‡æ ·æœ¬ {valid_samples[idx]} (æ–¹æ³• {method_name} ä¸åœ¨æŒ‡å®šåˆ—è¡¨ä¸­)")
            log(f"è·³è¿‡æ ·æœ¬ {valid_samples[idx]} (æ–¹æ³• {method_name} ä¸åœ¨æŒ‡å®šåˆ—è¡¨ä¸­)")
            continue

        for attempt in range(max_retries + 1):
            try:
                print(f"[Info] æ­£åœ¨å¤„ç†æ ·æœ¬: {valid_samples[idx]} (ç¬¬{attempt+1}æ¬¡å°è¯•)")
                log(f"æ­£åœ¨å¤„ç†æ ·æœ¬: {valid_samples[idx]} (ç¬¬{attempt+1}æ¬¡å°è¯•)")
                # è°ƒç”¨å¤§æ¨¡å‹æ‰¹é‡æ‰“åˆ†ï¼Œè¿”å›ç»“æœã€åŸå§‹responseã€å¤æŸ¥æ—¥å¿—
                res, gpt_raw, gpt_log = evaluator.process_batch(
                    [sample],
                    return_gpt_response=True,
                    target_domains=TARGET_DOMAINS
                )
                all_results.append(res)
                all_gpt_raw_responses.append(gpt_raw[0])
                all_gpt_logs.append(gpt_log)
                # è®°å½•æˆåŠŸæ ·æœ¬ä¸ç»“æœ
                successful_results.append((sample_names[idx], res))
                break
            except RecursionError as e:
                # ä¸“é—¨æ•è·é€’å½’é”™è¯¯ï¼Œæ‰“å°å®Œæ•´å †æ ˆå¹¶è·³è¿‡è¯¥æ ·æœ¬
                print(f"[Fatal] å¤„ç†æ ·æœ¬ {valid_samples[idx]} æ—¶å‡ºç° RecursionError: {e}")
                log(f"å¤„ç†æ ·æœ¬ {valid_samples[idx]} æ—¶å‡ºç° RecursionError: {e}")
                traceback.print_exc()
                print("[Error] é‡åˆ°é€’å½’æ·±åº¦é—®é¢˜ï¼Œå·²è·³è¿‡è¯¥æ ·æœ¬ã€‚è‹¥æ–¹ä¾¿ï¼Œè¯·å°†å®Œæ•´ traceback å¤åˆ¶å‡ºæ¥ç”¨äºè¿›ä¸€æ­¥å®šä½ã€‚")
                log("é‡åˆ°é€’å½’æ·±åº¦é—®é¢˜ï¼Œå·²è·³è¿‡è¯¥æ ·æœ¬ã€‚")
                break
            except Exception as e:
                print(f"[Error] å¤„ç†æ ·æœ¬ {valid_samples[idx]} å¤±è´¥: {e}")
                log(f"å¤„ç†æ ·æœ¬ {valid_samples[idx]} å¤±è´¥: {e}")
                if attempt == max_retries:
                    print(f"[Error] å·²é‡è¯•{max_retries+1}æ¬¡ï¼Œè·³è¿‡è¯¥æ ·æœ¬ã€‚è¯·æŸ¥çœ‹output_logè·å–è¯¦ç»†ä¿¡æ¯ã€‚")
                    log(f"å·²é‡è¯•{max_retries+1}æ¬¡ï¼Œè·³è¿‡è¯¥æ ·æœ¬ã€‚")
                else:
                    print("[Info] æ­£åœ¨é‡è¯•...")
                    log("æ­£åœ¨é‡è¯•...")
except Exception as e:
    print(f"[Fatal] æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}ï¼Œè¯·æŸ¥çœ‹output_logè·å–è¯¦ç»†ä¿¡æ¯ã€‚")
    log(f"æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
    exit(1)

# æ‰“å°å¤§æ¨¡å‹åŸå§‹response
print("\n===== GPT Raw Response (English) =====")
for idx, resp in enumerate(all_gpt_raw_responses):
    print(f"\n[Sample {idx+1} GPT Response]:\n{resp}\n")
print("===== End of GPT Raw Response =====\n")
log("å…¨éƒ¨æ ·æœ¬å¤„ç†å®Œæˆã€‚")

# --- è¾“å‡ºæ–‡ä»¶åä»¥è¾“å…¥csvåä¸ºå‰ç¼€ ---
if len(successful_results) == 1:
    prefix, result = successful_results[0]
    # å¦‚æœæ˜¯å•/å°‘æ•° domain é‡è·‘ï¼Œè¿›è¡Œå¢é‡åˆå¹¶ï¼šåªè¦†ç›–æŒ‡å®š domainï¼Œå…¶ä½™ domain ä½¿ç”¨å†å²å¾—åˆ†
    merged_result = merge_domain_results_incremental(prefix, result, output_dir, TARGET_DOMAINS)
    df_result = pd.DataFrame(merged_result['best_domains']) if 'best_domains' in merged_result else pd.DataFrame(merged_result)
    write_output(merged_result, df_result, prefix, output_dir, args.output_format, log_func=log, sql_db=args.sql_db, skip_existing=args.skip_existing)
    ext = {'json': 'json', 'dataframe': 'csv', 'excel': 'xlsx', 'sql': 'db', 'markdown': 'md'}[args.output_format]
    output_file = os.path.join(output_dir, f'{prefix}_result.{ext}')
    output_files.append(output_file)
elif len(successful_results) > 1:
    for prefix, result in successful_results:
        merged_result = merge_domain_results_incremental(prefix, result, output_dir, TARGET_DOMAINS)
        df_result = pd.DataFrame(merged_result['best_domains']) if 'best_domains' in merged_result else pd.DataFrame(merged_result)
        write_output(merged_result, df_result, prefix, output_dir, args.output_format, log_func=log, sql_db=args.sql_db, skip_existing=args.skip_existing)
        ext = {'json': 'json', 'dataframe': 'csv', 'excel': 'xlsx', 'sql': 'db', 'markdown': 'md'}[args.output_format]
        output_file = os.path.join(output_dir, f'{prefix}_result.{ext}')
        output_files.append(output_file)
else:
    print('[Warning] æ‰€æœ‰æ ·æœ¬å‡å¤„ç†å¤±è´¥ï¼Œæœªç”Ÿæˆä»»ä½•ä¸»ç¨‹åºè¾“å‡ºæ–‡ä»¶ã€‚')
    log('æ‰€æœ‰æ ·æœ¬å‡å¤„ç†å¤±è´¥ï¼Œæœªç”Ÿæˆä»»ä½•ä¸»ç¨‹åºè¾“å‡ºæ–‡ä»¶ã€‚')

if output_files:
    for f in output_files:
        print(f'å·²ç”Ÿæˆ {f} åˆ°outputæ–‡ä»¶ä¸­')
else:
    print('æœªç”Ÿæˆä»»ä½•è¾“å‡ºæ–‡ä»¶ï¼Œè¯·æŸ¥çœ‹output_logè·å–è¯¦ç»†ä¿¡æ¯ã€‚')

# æ·»åŠ æ„å»ºçŸ©é˜µçš„åŠŸèƒ½å‡½æ•°
def load_domain_scores(result_json: str) -> Dict[int, float]:
    """return mapping domain_index -> total score (0-1)."""
    with open(result_json, "r", encoding="utf-8") as f:
        data = json.load(f)
    doms = data.get("best_domains", data.get("domains", []))
    m = {}
    for d in doms:
        # ä¼˜å…ˆä½¿ç”¨sub_scoresçš„ç²¾ç¡®æ€»å’Œï¼Œæä¾›æ›´å¥½çš„åˆ†æ•°åŒºåˆ†åº¦ï¼ˆå¢å¼ºç‰ˆï¼‰
        if "sub_scores" in d and isinstance(d["sub_scores"], list) and len(d["sub_scores"]) > 0:
            # ä½¿ç”¨é«˜ç²¾åº¦è®¡ç®—é¿å…æµ®ç‚¹è¯¯å·®
            getcontext().prec = 12

            # å°†å­åˆ†æ•°è½¬æ¢ä¸ºé«˜ç²¾åº¦Decimalè¿›è¡Œè®¡ç®—
            sub_scores = [Decimal(str(s)) for s in d["sub_scores"]]
            val = float(sum(sub_scores))

            # ä¿ç•™æ›´é«˜ç²¾åº¦å‡å°‘åŒåˆ†ï¼šä¿ç•™8ä½å°æ•°
            val = round(val, 8)
            
            # æ·»åŠ ç¡®å®šæ€§å¾®æ‰°é¿å…å®Œå…¨åŒåˆ†
            domain_idx = int(d["domain_index"])
            perturbation = ((domain_idx * 2654435761) % 997) * 1e-6  # ç¡®å®šæ€§å¾®æ‰°ï¼Œå¢å¤§åˆ°1e-6
            val += perturbation
            
        else:
            # å›é€€åˆ°é¢„è®¾çš„scoreæˆ–totalå­—æ®µ
            val = d.get("score", d.get("total", 0))
            val = float(val)
            
            # ä¸ºå›é€€åˆ†æ•°ä¹Ÿæ·»åŠ å¾®æ‰°
            domain_idx = int(d["domain_index"])
            perturbation = ((domain_idx * 2654435761) % 997) * 1e-6  # ç¡®å®šæ€§å¾®æ‰°ï¼Œå¢å¤§åˆ°1e-6
            val += perturbation

        # ç»Ÿä¸€ä½¿ç”¨0-1åˆ†åˆ¶ï¼Œå¦‚æœåˆ†æ•°>1ï¼Œåˆ™æŠ¥é”™æç¤ºéœ€è¦è½¬æ¢
        if val > 1:
            raise ValueError(f"Score {val} is greater than 1. Please ensure all scores are in 0-1 range.")

        m[domain_idx] = val
    return m


def build_matrices(methods: List[str], spot_csvs: List[str], result_jsons: List[str]) -> (pd.DataFrame, pd.DataFrame):
    # Collect all spot_ids
    all_spots = set()
    per_method_scores = {}
    per_method_labels = {}

    for m, csv_path, res_path in zip(methods, spot_csvs, result_jsons):
        # æ£€æŸ¥CSVæ–‡ä»¶ç»“æ„
        header_df = pd.read_csv(csv_path, nrows=0)
        
        if "spot_id" in header_df.columns:
            # å¦‚æœæœ‰æ˜ç¡®çš„spot_idåˆ—
            spot_df = pd.read_csv(csv_path)
            spot_ids = spot_df["spot_id"].astype(str)
        elif "Unnamed: 0" in header_df.columns:
            # ç¬¬ä¸€åˆ—åŒ…å«spot_idä½†åä¸º"Unnamed: 0"
            spot_df = pd.read_csv(csv_path)
            spot_ids = spot_df["Unnamed: 0"].astype(str)
            # é‡å‘½åç¬¬ä¸€åˆ—ä¸ºspot_id
            spot_df = spot_df.rename(columns={"Unnamed: 0": "spot_id"})
        else:
            # ç¬¬ä¸€åˆ—æ˜¯ç´¢å¼•ï¼Œç›´æ¥è®¾ä¸ºspot_id
            spot_df = pd.read_csv(csv_path, index_col=0)
            spot_ids = spot_df.index.astype(str)
            spot_df = spot_df.reset_index()
            spot_df = spot_df.rename(columns={spot_df.columns[0]: 'spot_id'})
            
        # æ ‡å‡†åŒ–åˆ—å
        if "domain_index" not in spot_df.columns and "spatial_domain" in spot_df.columns:
            spot_df = spot_df.rename(columns={"spatial_domain": "domain_index"})

        # ensure domain_index is integer so it matches JSON keys
        spot_df["domain_index"] = spot_df["domain_index"].astype(int)

        all_spots.update(spot_ids)

        # åˆ›å»ºspot_idåˆ°domain_indexçš„æ˜ å°„
        spot_to_domain = dict(zip(spot_ids, spot_df["domain_index"]))
        per_method_labels[m] = pd.Series(spot_to_domain)

        # Map domain score
        score_map = load_domain_scores(res_path)
        per_method_scores[m] = pd.Series(spot_to_domain).map(score_map)

    # æ­£ç¡®çš„spotæ’åºï¼šå¯¹äºbarcodeä½¿ç”¨å­—å…¸åº
    def sort_spot_ids(spot_ids):
        """æ­£ç¡®æ’åºspot_idï¼šbarcodeä½¿ç”¨å­—å…¸åºï¼Œæ•°å­—ä½¿ç”¨æ•°å€¼åº"""
        spot_list = list(spot_ids)
        if not spot_list:
            return spot_list
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯barcodeæ ¼å¼ï¼ˆåŒ…å«å­—æ¯å’Œè¿å­—ç¬¦ï¼‰
        sample_spot = spot_list[0]
        if '-' in sample_spot and any(c.isalpha() for c in sample_spot):
            # barcodeæ ¼å¼ï¼Œä½¿ç”¨å­—å…¸åº
            return sorted(spot_list)
        else:
            # æ•°å­—æ ¼å¼ï¼Œå°è¯•æ•°å€¼æ’åº
            try:
                return sorted(spot_list, key=lambda x: int(x))
            except ValueError:
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå›é€€åˆ°å­—å…¸åº
                return sorted(spot_list)

    all_spots = sort_spot_ids(all_spots)

    # Build DataFrames with all spots
    scores_df = pd.DataFrame(index=all_spots)
    labels_df = pd.DataFrame(index=all_spots)
    for m in methods:
        scores_df[m] = per_method_scores[m].reindex(all_spots)
        labels_df[m] = per_method_labels[m].reindex(all_spots)

    # å¡«è¡¥ç¼ºå¤±åŸŸï¼šå¦‚æœæŸæ–¹æ³•æ²¡æœ‰ç‰¹å®šdomainï¼Œç”¨ -1 å ä½ï¼Œé¿å…åˆ—æ•´ä½“ç¼ºå¤±
    # æ³¨æ„ï¼šåªè½¬æ¢åˆ—æ•°æ®ç±»å‹ï¼Œä¸å½±å“ç´¢å¼•
    for col in labels_df.columns:
        labels_df[col] = labels_df[col].fillna(-1).astype(int)

    # æ‰€æœ‰åˆ†æ•°åº”è¯¥å·²ç»æ˜¯0-1èŒƒå›´ï¼Œä¸éœ€è¦é¢å¤–çš„ç¼©æ”¾å¤„ç†
    
    # ä¿æŒåŸå§‹çš„spot_idä½œä¸ºç´¢å¼•ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
    # ä¸å†é‡æ–°ç´¢å¼•ä¸ºæ•°å­—ï¼Œè¿™æ ·å¯ä»¥ä¿æŒspot_idä¸spatial_domainçš„æ­£ç¡®å¯¹åº”å…³ç³»

    return scores_df, labels_df


def generate_domain_choice_record(scores_df: pd.DataFrame, labels_df: pd.DataFrame, output_dir: str, log_func):
    """
    ç”Ÿæˆdomainé€‰æ‹©è®°å½•æ–‡ä»¶ï¼Œè®°å½•æ¯ä¸ªdomainçš„æœ€é«˜åˆ†æ•°å’Œå¯¹åº”æ–¹æ³•
    
    Args:
        scores_df: åˆ†æ•°çŸ©é˜µDataFrame
        labels_df: æ ‡ç­¾çŸ©é˜µDataFrame  
        output_dir: è¾“å‡ºç›®å½•
        log_func: æ—¥å¿—å‡½æ•°
    """
    try:
        domain_choices = {}
        
        # éå†æ¯ä¸ªspotï¼Œæ‰¾åˆ°æœ€é«˜åˆ†æ•°å’Œå¯¹åº”çš„æ–¹æ³•
        for spot_id in scores_df.index:
            spot_scores = scores_df.loc[spot_id]
            spot_labels = labels_df.loc[spot_id]
            
            # è·³è¿‡å…¨ä¸ºNaNçš„è¡Œ
            if spot_scores.isna().all():
                continue
                
            # æ‰¾åˆ°æœ€é«˜åˆ†æ•°çš„æ–¹æ³•
            max_score_method = spot_scores.idxmax()
            max_score = spot_scores.max()
            domain_label = spot_labels[max_score_method]
            
            # è·³è¿‡NaNå€¼
            if pd.isna(max_score) or pd.isna(domain_label):
                continue
                
            domain_key = f"Domain_{int(domain_label)}"
            
            # å¦‚æœè¿™ä¸ªdomainè¿˜æ²¡æœ‰è®°å½•ï¼Œæˆ–è€…å½“å‰åˆ†æ•°æ›´é«˜ï¼Œåˆ™æ›´æ–°è®°å½•
            if domain_key not in domain_choices or max_score > domain_choices[domain_key]["score"]:
                domain_choices[domain_key] = {
                    "score": float(max_score),
                    "method": max_score_method,
                    "domain_index": int(domain_label),
                    "representative_spot": spot_id
                }
        
        # æŒ‰domainç¼–å·æ’åº
        sorted_choices = dict(sorted(domain_choices.items(), 
                                   key=lambda x: x[1]["domain_index"]))
        
        # æ·»åŠ æ±‡æ€»ä¿¡æ¯
        summary = {
            "total_domains": len(sorted_choices),
            "methods_used": list(set(choice["method"] for choice in sorted_choices.values())),
            "average_score": sum(choice["score"] for choice in sorted_choices.values()) / len(sorted_choices) if sorted_choices else 0,
            "generation_info": {
                "total_spots_analyzed": len(scores_df),
                "methods_available": list(scores_df.columns),
                "timestamp": pd.Timestamp.now().isoformat()
            }
        }
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        result = {
            "summary": summary,
            "domain_choices": sorted_choices
        }
        
        # ä¿å­˜åˆ°outputç›®å½•ä¸‹çš„domain_choice.json
        choice_file_path = os.path.join(output_dir, "domain_choice.json")
        with open(choice_file_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        log_func(f"Domainé€‰æ‹©è®°å½•å·²ç”Ÿæˆ: {choice_file_path}")
        log_func(f"  - æ€»åŸŸæ•°: {summary['total_domains']}")
        log_func(f"  - ä½¿ç”¨çš„æ–¹æ³•: {', '.join(summary['methods_used'])}")
        log_func(f"  - å¹³å‡åˆ†æ•°: {summary['average_score']:.3f}")
        
        print(f"[Info] Domainé€‰æ‹©è®°å½•å·²ç”Ÿæˆ: {choice_file_path}")
        print(f"  - æ€»åŸŸæ•°: {summary['total_domains']}")
        print(f"  - ä½¿ç”¨çš„æ–¹æ³•: {', '.join(summary['methods_used'])}")
        print(f"  - å¹³å‡åˆ†æ•°: {summary['average_score']:.3f}")
        
        return True
        
    except Exception as e:
        log_func(f"ç”Ÿæˆdomainé€‰æ‹©è®°å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(f"[Warning] ç”Ÿæˆdomainé€‰æ‹©è®°å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False


def auto_build_matrices(input_dir: str, output_dir: str, valid_samples: List[str], log_func, skip_existing: bool = False):
    """è‡ªåŠ¨æ„å»ºçŸ©é˜µï¼ŒåŸºäºå·²æœ‰çš„è¾“å…¥æ–‡ä»¶å’Œè¾“å‡ºç»“æœ"""
    try:
        # ä¿å­˜çŸ©é˜µæ–‡ä»¶è·¯å¾„ï¼ˆä¸ä¸»è¾“å‡ºç›®å½•å¯¹é½ï¼‰
        # ç»Ÿä¸€è¾“å‡ºåˆ°: output/consensus/
        matrix_output_dir = os.path.join(output_dir, "consensus")
        os.makedirs(matrix_output_dir, exist_ok=True)
        
        scores_path = os.path.join(matrix_output_dir, "scores_matrix.csv")
        labels_path = os.path.join(matrix_output_dir, "labels_matrix.csv")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨çŸ©é˜µæ–‡ä»¶ï¼ˆåªæœ‰åœ¨skip_existing=Trueæ—¶æ‰è·³è¿‡ï¼‰
        if skip_existing and os.path.exists(scores_path) and os.path.exists(labels_path):
            log_func(f"çŸ©é˜µæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æ„å»ºï¼š")
            log_func(f"  - åˆ†æ•°çŸ©é˜µ: {scores_path}")
            log_func(f"  - æ ‡ç­¾çŸ©é˜µ: {labels_path}")
            print(f"[Info] çŸ©é˜µæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æ„å»º")
            return True, matrix_output_dir
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨çš„æ–¹æ³•ï¼ˆåŸºäºè¾“å…¥æ–‡ä»¶ï¼‰
        methods = []
        spot_csvs = []
        result_jsons = []

        for sample in valid_samples:
            # æ„å»ºè¾“å…¥CSVæ–‡ä»¶è·¯å¾„
            spot_csv = os.path.join(input_dir, f"{sample}_spot.csv")
            result_json = os.path.join(output_dir, f"{sample}_result.json")

            if os.path.exists(spot_csv) and os.path.exists(result_json):
                # ä»sampleåç§°ä¸­æå–æ–¹æ³•åï¼ˆå‡è®¾æ ¼å¼ä¸º METHOD_DATASET_IDï¼‰
                method_name = sample.split('_')[0]  # ä¾‹å¦‚ä» "GraphST_DLPFC_151507" æå– "GraphST"
                methods.append(method_name)
                spot_csvs.append(spot_csv)
                result_jsons.append(result_json)

        if len(methods) < 2:
            log_func(f"éœ€è¦è‡³å°‘2ä¸ªæ–¹æ³•æ‰èƒ½æ„å»ºçŸ©é˜µï¼Œå½“å‰åªæœ‰{len(methods)}ä¸ªæœ‰æ•ˆæ–¹æ³•")
            return False, None

        log_func(f"æ­£åœ¨ä¸ºä»¥ä¸‹æ–¹æ³•æ„å»ºçŸ©é˜µ: {', '.join(methods)}")

        # æ„å»ºçŸ©é˜µ
        scores_df, labels_df = build_matrices(methods, spot_csvs, result_jsons)

        # ä¿å­˜çŸ©é˜µæ–‡ä»¶ï¼ˆé»˜è®¤è¦†ç›–ï¼‰
        scores_df.to_csv(scores_path)
        labels_df.to_csv(labels_path)

        log_func(f"çŸ©é˜µæ„å»ºå®Œæˆ:")
        log_func(f"  - åˆ†æ•°çŸ©é˜µ: {scores_path}")
        log_func(f"  - æ ‡ç­¾çŸ©é˜µ: {labels_path}")
        
        print(f"[Info] çŸ©é˜µæ„å»ºå®Œæˆ:")
        print(f"  - åˆ†æ•°çŸ©é˜µ: {scores_path}")
        print(f"  - æ ‡ç­¾çŸ©é˜µ: {labels_path}")
        
        # ç”Ÿæˆdomainé€‰æ‹©è®°å½•
        choice_success = generate_domain_choice_record(scores_df, labels_df, output_dir, log_func)
        if choice_success:
            log_func("Domainé€‰æ‹©è®°å½•ç”ŸæˆæˆåŠŸ")
        else:
            log_func("Domainé€‰æ‹©è®°å½•ç”Ÿæˆå¤±è´¥")
        
        return True, matrix_output_dir

    except Exception as e:
        log_func(f"æ„å»ºçŸ©é˜µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(f"[Warning] æ„å»ºçŸ©é˜µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False, None

# ARIè®¡ç®—å‡½æ•°å·²ç§»é™¤
# åŸæ¥çš„calculate_ari_and_visualize, select_best_labels, 
# get_neighbor_indices, smooth_spatial_domainsç­‰å‡½æ•°å·²åˆ é™¤



# è‡ªåŠ¨æ„å»ºçŸ©é˜µï¼ˆå¦‚æœå¯ç”¨äº†è¯¥é€‰é¡¹ä¸”æœ‰å¤šä¸ªæ–¹æ³•ï¼‰
if args.build_matrices and not args.no_build_matrices:
    if len(valid_samples) >= 2:
        print("[Info] æ­£åœ¨å°è¯•æ„å»ºçŸ©é˜µ...")
        log("æ­£åœ¨å°è¯•æ„å»ºçŸ©é˜µ...")
        
        # æ„å»ºçŸ©é˜µ
        matrix_success, matrix_output_dir = auto_build_matrices(input_dir, output_dir, valid_samples, log, args.skip_existing)
        
        # ARIè®¡ç®—åŠŸèƒ½å·²ç§»é™¤
        if matrix_success:
            print("[Info] çŸ©é˜µæ„å»ºæˆåŠŸ")
            log("çŸ©é˜µæ„å»ºæˆåŠŸ")
        else:
            print("[Warning] çŸ©é˜µæ„å»ºå¤±è´¥")
            log("çŸ©é˜µæ„å»ºå¤±è´¥")
    else:
        print("[Info] åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œè·³è¿‡çŸ©é˜µæ„å»º")
        log("åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œè·³è¿‡çŸ©é˜µæ„å»º")
else:
    print("[Info] çŸ©é˜µæ„å»ºè¢«ç¦ç”¨")
    log("çŸ©é˜µæ„å»ºè¢«ç¦ç”¨")

print("\n" + "="*60)
print("ğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
print("="*60)
print("ğŸ“ æ–‡ä»¶å¤„ç†è¯´æ˜:")
if args.skip_existing:
    print("  âœ… è·³è¿‡æ¨¡å¼ï¼šå·²å­˜åœ¨çš„æ–‡ä»¶å°†è¢«è·³è¿‡")
elif args.clean_output:
    print("  ğŸ”„ æ¸…ç†æ¨¡å¼ï¼šæ‰€æœ‰æ–‡ä»¶å·²é‡æ–°ç”Ÿæˆ")
else:
    print("  ğŸ”„ è¦†ç›–æ¨¡å¼ï¼šçŸ©é˜µæ–‡ä»¶å·²æ›´æ–°ï¼ˆä¸»ç¨‹åºç»“æœæ–‡ä»¶å¦‚å·²å­˜åœ¨åˆ™è·³è¿‡ï¼‰")


print("\nğŸ’¡ å¸¸ç”¨å‘½ä»¤å‚è€ƒ:")
print("  python scoring.py                     # å®Œæ•´æµç¨‹ï¼ˆè¯„åˆ†â†’çŸ©é˜µç”Ÿæˆï¼‰")
print("  python scoring.py --skip_existing     # è·³è¿‡æ‰€æœ‰å·²å­˜åœ¨æ–‡ä»¶")
print("  python scoring.py --clean_output      # æ¸…ç†åé‡æ–°ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶")
print("  python scoring.py --no_build_matrices # ä»…è¿è¡Œä¸»ç¨‹åºï¼Œä¸æ„å»ºçŸ©é˜µ")
print("="*60)

 